{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJ2eOwDi3Y58"
   },
   "source": [
    "# **실습11. Attention 기반 seq2seq**  \n",
    "\n",
    "> 본 스켈레톤 코드 파일은 실습 위주로 다룰 것이므로, attention에 대한 개념 이해는 본책 3권의 '0. basic seq2seq의 한계', '1. 도식으로 Attention의 원리 이해하기' 파트를 활용하도록 하자. 본 실습 스켈레톤 코드 파일에서는 '2. 수식과 핵심 코드로 attention 개념 심화하기'로 넘어가볼 것이다.\n",
    "\n",
    "> 11.0. 과 11.1. 절의 내용은 따로 제공된 Remnote 파일을 참고하기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ih1W4hbaPQV"
   },
   "source": [
    "## 1. 필요한 함수 Import\n",
    "실습을 시작하기 전, 실습 10에서 정의한 함수들과 클래스를 import 하자. 아래 Cell 은 그대로 실행하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "h4m6Kek6aPQV"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mynn.layers.dense import dense\n",
    "from mynn.optimizers.adam import Adam\n",
    "\n",
    "from mygrad.nnet.losses import softmax_crossentropy\n",
    "from mygrad.nnet.initializers import glorot_normal\n",
    "from mygrad.nnet.activations import relu\n",
    "\n",
    "import mygrad as mg\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "C_9MbdRYaPQW"
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, dim_input, dim_recurrent, dim_output):\n",
    "        \"\"\" RNN에 필요한 모든 레이어 초기 설정\n",
    "        \n",
    "        매개변수\n",
    "        ----------\n",
    "        dim_input: int \n",
    "            RNN을 지나가는 데이터의 차원 (C)\n",
    "        \n",
    "        dim_recurrent: int\n",
    "            RNN에 있는 hidden state의 차원 (D)\n",
    "        \n",
    "        dim_output: int\n",
    "            RNN의 output의 차원 (K)\n",
    "        \"\"\"\n",
    "        self.fc_x2h = dense(dim_input, dim_recurrent,  weight_initializer=glorot_normal)\n",
    "        self.fc_h2h = dense(dim_recurrent, dim_recurrent, weight_initializer=glorot_normal, bias=False)\n",
    "        self.fc_h2y = dense(dim_recurrent, dim_output, weight_initializer=glorot_normal)\n",
    "    \n",
    "    def __call__(self, x, h=None):\n",
    "        \"\"\" RNN에 대한 완전한 순방향 패스(full forward pass)를 수행.\n",
    "        \n",
    "        매개변수\n",
    "        ----------\n",
    "        x: Union[numpy.ndarray, mygrad.Tensor], shape=(T, N, C)\n",
    "            batch의 각 시퀀스에 대한 one-hot 인코딩\n",
    "        \n",
    "        h: Optional[Union[numpy.ndarray, mygrad.Tensor]], shape=(1, N, D)\n",
    "            초기 hidden dimension state h_0에 대한 옵션 설정\n",
    "            지정되지 않을 경우 영행렬 할당.\n",
    "        \n",
    "        반환 값\n",
    "        -------\n",
    "        Tuple[y, h]\n",
    "            y: mygrad.Tensor, shape=(T, N, K)\n",
    "                각 RNN step에 대한 최종 분류 점수\n",
    "            h: mygrad.Tensor, shape=(T, N, D)\n",
    "                각 RNN step에서 계산된 hidden states (초기 상태 h_0는 제외)\n",
    "        \"\"\"\n",
    "        N = x.shape[1]\n",
    "        D = self.fc_h2h.weight.shape[0]\n",
    "        \n",
    "        h_t = np.zeros((1, N, D), dtype=np.float32) if h is None else h\n",
    "        h = []\n",
    "        \n",
    "        for x_t in x:\n",
    "            h_t = relu(self.fc_x2h(x_t[np.newaxis]) + self.fc_h2h(h_t))\n",
    "            h.append(h_t)\n",
    "        \n",
    "        h = mg.concatenate(h, axis=0)\n",
    "        \n",
    "        return self.fc_h2y(h), h\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" 모델 내 모든 매개변수를 보여주는 편리한 함수\n",
    "        이것은 `model.parameters`를 통해 속성(attribute)으로 액세스 가능\n",
    "        \n",
    "        반환 값\n",
    "        -------\n",
    "        Tuple[Tensor, ...]\n",
    "            우리 모델에 대한 모든 학습 가능한 매개변수를 포함하는 튜플\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.fc_x2h.parameters + self.fc_h2h.parameters + self.fc_h2y.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "d0uughSZaPQX"
   },
   "outputs": [],
   "source": [
    "def generate_batch(seq_len_min=1, seq_len_max=20, batch_size=10):\n",
    "    T_1 = np.random.randint(seq_len_min, seq_len_max + 1)\n",
    "    digits = np.random.randint(0, 10, (T_1, batch_size))\n",
    "    one_hot_x = np.zeros((T_1 + 1, batch_size, 12), dtype=np.float32)\n",
    "    one_hot_x[np.arange(T_1).reshape(-1, 1), np.arange(batch_size), digits] = 1\n",
    "    one_hot_x[-1, :, -1] = 1\n",
    "    ends = np.full(batch_size, 11).reshape(1, -1)\n",
    "    y = np.concatenate([digits[::-1], ends], axis=0)\n",
    "    return one_hot_x, y, digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QgRFxMh-aPQY"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode_prediction(y_t):\n",
    "    \"\"\" 분류 점수 y_t에 대한 하나의 batch를 입력받으면, 가장 높은 점수를 가진 클래스에 해당하는 원-핫 인코딩을 반환하는 함수.\n",
    "    \n",
    "    매개변수\n",
    "    ----------\n",
    "    y_t: mygrad.Tensor, shape=(1, N, K)\n",
    "        seq2seq 디코딩의 한 step으로부터 예측된 분류 점수\n",
    "    \n",
    "    반환 값\n",
    "    -------\n",
    "    s_t1: numpy.ndarray, shape=(1, N, K), dtype=np.float32\n",
    "        y_t에서의 가장 큰 예측 점수에 해당하는 레이블의 one-hot 인코딩. \n",
    "        float32의 NumPy 배열.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    N : batch size\n",
    "    K : 클래스 개수 (i.e. vocabulary의 크기)\n",
    "    \"\"\"\n",
    "    # 여기에 코드 작성\n",
    "    y_t = y_t.data\n",
    "    max_values_mask = (y_t == y_t.max(axis=-1, keepdims=True))\n",
    "\n",
    "    return max_values_mask.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "by5fko1j31W9"
   },
   "source": [
    "## **2. 수식과 핵심 코드로 attention 개념 심화하기**\n",
    "\n",
    "### **2.1 수식으로 attention score 개념 다지기**\n",
    "\n",
    "attention 메커니즘의 핵심은 바로 attention score이다. attention score란, 디코딩 과정에서 인코더의 어느 입력 step에 주목할 것인지의 가중치를 점수로 나타낸 값을 말한다. 디코딩 과정이 t개의 step들로 나뉘어 있다고 생각해보자. (한 step마다 하나의 output을 만든다.) 앞서 언급했듯, attention scores는 ‘인코더의 모든 hidden states’와 ‘현재 디코더의 hidden descriptor’ 두 가지를 모두 고려하여 계산된다. 현재 decoder가 t번째 step에 있다면, attention scores(=$e_t$)의 계산에 필요한 재료들은 이하와 같다. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**attention score** =  $\\begin{equation}\n",
    "\\vec{e}_t = H^e W_\\alpha \\vec{h}{}^d_t\n",
    "\\end{equation}$  \n",
    ">(1) $H^e$: 인코더의 모든 hidden descriptor 모음\n",
    ">D-차원의 모든 T개의 인코더 hidden descriptors 행렬 (T,D)  \n",
    "$\\begin{align}\n",
    "&\\:\\begin{matrix}\\xleftarrow{\\hspace{0.75em}} & D & \\xrightarrow{\\hspace{0.75em}}\\end{matrix} \\\\\n",
    "H^e =\\;\\, &\\begin{bmatrix}\\leftarrow & \\vec{h}{}^e_1 & \\rightarrow \\\\ \\leftarrow & \\vec{h}{}^e_2 & \\rightarrow \\\\ \\vdots & \\vdots & \\vdots \\\\ \\leftarrow & \\vec{h}{}^e_T & \\rightarrow\\end{bmatrix}\\;\\;\\begin{matrix}\\bigg\\uparrow \\\\ T \\\\ \\bigg\\downarrow\\end{matrix}\n",
    "\\end{align}$  \n",
    "\n",
    ">(2) $W_a$: 학습 가능한 매개변수인 가중치에 대한 행렬 (D,D)  \n",
    ">$\\begin{align}\n",
    "&\\;\\begin{matrix}\\xleftarrow{\\hspace{2.25em}} & D & \\xrightarrow{\\hspace{2.25em}}\\end{matrix} \\\\\n",
    "W_\\alpha =\\;\\, &\\begin{bmatrix}\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\ \\vec{W}_1 & \\vec{W}_2 & \\cdots & \\vec{W}_D \\\\ \\downarrow & \\downarrow & \\cdots & \\downarrow\\end{bmatrix}\\;\\;\\begin{matrix}\\big\\uparrow \\\\ D \\\\ \\big\\downarrow\\end{matrix}\n",
    "\\end{align}$  \n",
    "\n",
    ">(3) ${h_t}^d$: $t$ step에서의 디코더 hidden descriptor (D-차원)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mCwdKMW5kLT"
   },
   "source": [
    "위 수식을 달리 설명하면, t step에서의 attention scores인 $e_t$는 ‘${h_t}^d$가 확장된 형태’라고도 볼 수 있다. 이때 디코더의 hidden descriptor는 인코더의 모든 descriptors를 검토한 후 학습을 통해 자신과 가장 ‘연관 있는’ 인코더 토큰이 무엇인지 판단한다. 그리고 모델은 학습을  더 많이 진행함에 따라 ${h_t}^d$와 각 인코더 hidden descriptor 간의 관련도를 더 정확하게 파악하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMjVNSd-5sXt"
   },
   "source": [
    "### **2.2 디코더 hidden descriptor의 학습 과정 이해하기**\n",
    "\n",
    "attention score 계산 수식인 $e_t = H^eW_a{h_t}^d$을 기반으로, ${h_t}^d$가 학습을 통해 어떻게 개선되는지 그 과정을 수식과 코드로 구체적으로 알아보자.  \n",
    " ① $H^eW_\\alpha$에 대한 행렬곱부터 시작해보자. $H^eW_\\alpha$의 각 value는 인코더 hidden state와 가중치 벡터 간의 ‘유사성'을 나타내게 된다.  \n",
    " \n",
    " $\\begin{align}\n",
    "&\\;\\begin{matrix}\\xleftarrow{\\hspace{4.75em}} & D & \\xrightarrow{\\hspace{4.75em}}\\end{matrix} \\\\\n",
    "H^eW_\\alpha =\\;\\, &\\begin{bmatrix}\\vec{h}{}^e_1\\cdot\\vec{W}_1 & \\vec{h}{}^e_1\\cdot\\vec{W}_2 & \\cdots & \\vec{h}{}^e_1\\cdot\\vec{W}_D \\\\ \\vec{h}{}^e_2\\cdot\\vec{W}_1 & \\vec{h}{}^e_2\\cdot\\vec{W}_2 & \\cdots & \\vec{h}^e_2\\cdot\\vec{W}_D \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\vec{h}{}^e_T\\cdot\\vec{W}_1 & \\vec{h}{}^e_T\\cdot\\vec{W}_2 & \\cdots & \\vec{h}{}^e_T\\cdot\\vec{W}_D\\end{bmatrix}\\;\\;\\begin{matrix}\\bigg\\uparrow \\\\ T \\\\ \\bigg\\downarrow\\end{matrix}\n",
    "\\end{align}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention score를 softmax에 넣은 것이 attention weight가 된다.  \n",
    "attention weight을 다시 encoder의 hidden state의 가중치로 적용한 것이 context value이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.82741978, -0.25495778,  2.07219562, ...,  1.67168075,\n",
       "          0.82918854,  0.73916311],\n",
       "        [-1.1093314 , -1.22105985, -0.0343787 , ...,  1.07869315,\n",
       "          0.46189283,  0.26365016],\n",
       "        [-0.45485703,  0.09647406,  0.04878042, ..., -0.53379739,\n",
       "          1.05325315,  1.16490893],\n",
       "        ...,\n",
       "        [-0.66047107,  0.36036743,  0.12346351, ...,  0.96343911,\n",
       "         -0.50509705,  0.0841679 ],\n",
       "        [ 0.10642131,  0.31811769,  0.75550378, ..., -0.61734607,\n",
       "         -1.57064856, -0.2398009 ],\n",
       "        [-0.05764956, -0.63909454, -0.62375447, ...,  0.96431795,\n",
       "          0.71979522,  0.14311388]],\n",
       "\n",
       "       [[-1.25284386,  1.00537503,  0.7225561 , ..., -0.52064732,\n",
       "          0.02266663,  0.25401385],\n",
       "        [-0.34442585,  0.68109346, -1.06529843, ...,  1.22894868,\n",
       "         -0.17598339, -0.07129357],\n",
       "        [-2.95514763, -0.03805574,  0.34958339, ..., -0.21933287,\n",
       "          0.82777692, -1.30835971],\n",
       "        ...,\n",
       "        [-2.57254838,  0.87928048, -1.14891639, ..., -2.03537924,\n",
       "         -0.18863109,  0.702672  ],\n",
       "        [ 0.71581528, -0.18808255,  0.53271433, ..., -0.95341322,\n",
       "          1.82199884,  0.99784053],\n",
       "        [ 1.68188074, -1.29334063,  0.19004267, ...,  0.52848235,\n",
       "         -0.02082961, -0.41607819]],\n",
       "\n",
       "       [[-1.63001053,  0.82771433, -0.09838493, ...,  0.57166175,\n",
       "         -0.71690027, -0.8861434 ],\n",
       "        [-0.96034512, -0.46789466, -0.9394915 , ...,  1.61823279,\n",
       "          0.35012363, -0.87282595],\n",
       "        [-0.76470738, -1.46114418, -0.77892095, ..., -0.51087146,\n",
       "          0.62631374, -0.57433593],\n",
       "        ...,\n",
       "        [-0.75238351,  1.14436165, -0.94993206, ...,  2.09411728,\n",
       "         -1.76123001, -0.72930905],\n",
       "        [ 0.3857576 , -0.24945431, -0.38540295, ...,  0.39931169,\n",
       "          0.37112574, -2.11716168],\n",
       "        [-1.7881959 ,  1.32935304,  0.69863247, ...,  0.27699333,\n",
       "          1.54976029, -0.88794401]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.28896968, -0.03167583,  0.83748058, ...,  0.9381058 ,\n",
       "         -0.47067719, -0.32473061],\n",
       "        [-0.49872979, -1.51221188, -0.78437707, ..., -2.13039387,\n",
       "         -1.09272066,  0.57891421],\n",
       "        [ 0.29611138,  0.27206705, -0.47286643, ...,  0.05919917,\n",
       "          0.41301499,  1.93708041],\n",
       "        ...,\n",
       "        [-0.314981  , -0.10949436, -1.14446877, ...,  0.87498474,\n",
       "         -0.44035761,  0.79738739],\n",
       "        [-1.94197008,  1.43513041, -0.34479064, ...,  0.46584789,\n",
       "         -0.8914676 ,  0.42264412],\n",
       "        [-0.2117051 ,  0.73786722, -1.54509755, ...,  0.67364088,\n",
       "          1.74484371,  1.10298127]],\n",
       "\n",
       "       [[ 2.30839151,  1.34050365,  1.09484433, ...,  1.95801348,\n",
       "          0.23480212, -1.14853437],\n",
       "        [ 1.58959629,  2.06526151,  0.3911753 , ..., -0.6445697 ,\n",
       "          1.09122468, -1.51269646],\n",
       "        [-0.1948088 ,  0.41714141,  1.11233554, ..., -0.35817746,\n",
       "          0.9194945 , -1.34946105],\n",
       "        ...,\n",
       "        [ 0.56042638, -0.14941838, -0.9399582 , ..., -1.11804219,\n",
       "         -2.56685465,  0.35949568],\n",
       "        [ 0.55772511,  0.22829416, -0.76741394, ...,  0.25900045,\n",
       "          0.32587458, -1.02063498],\n",
       "        [-0.54166968, -0.23559743,  0.31930509, ...,  0.56091958,\n",
       "          0.34988016,  1.87307468]],\n",
       "\n",
       "       [[ 0.4032669 ,  0.0960118 ,  0.20281589, ..., -0.12106258,\n",
       "         -0.29990797, -0.62872881],\n",
       "        [-0.23372087, -1.37413953,  0.88209724, ...,  0.40296898,\n",
       "         -1.66548524, -0.30174068],\n",
       "        [ 0.63633957, -0.14237798, -0.05067778, ..., -0.72969238,\n",
       "         -1.08689255,  2.51303887],\n",
       "        ...,\n",
       "        [ 0.27021876, -0.08545439,  0.83113188, ..., -1.36090107,\n",
       "         -0.61970697,  0.66471351],\n",
       "        [ 1.54905506, -1.17295879,  1.32462876, ...,  1.4259652 ,\n",
       "          0.1355816 ,  0.96901523],\n",
       "        [-0.99869377, -0.37583719,  0.25497656, ...,  2.3573552 ,\n",
       "         -0.09701167, -0.21532025]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup - 이해를 위한 랜덤의 값 \n",
    "T = 10 # encoder에 들어가는 input sequence의 길이(length)\n",
    "N = 32 # batch size\n",
    "D = 18 # dimension of hidden state\n",
    "encoder_hidden_states = np.random.randn(T, N, D)\n",
    "encoder_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "pgVFKnPQ50PH",
    "outputId": "0f0614d5-021c-402e-ed2f-1bf4b98fe63d"
   },
   "outputs": [],
   "source": [
    "## H^eW_a = W_\\alpha @ H^e\n",
    "## 코드로 적용하면?\n",
    "\"\"\" \n",
    "위의 방법을 통해, 디코더를 호출할 때마다 다시 계산을 반복하는 것이 아니라, 단 한 번만으로 값들을 계산할 수 있게 된다. 각 디코더 단계 동안, 내적을 통해 최종 attention score 벡터를 계산할 수 있다.\n",
    "\"\"\" \n",
    "W_alpha = dense(D, D)\n",
    "precomputed_encoder_score_vectors = W_alpha(encoder_hidden_states)\n",
    "# (T, N, D) @ (D, D) -> (T, N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 32, 18)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precomputed_encoder_score_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZYXyYqp52Mq"
   },
   "source": [
    "② ${h_t}^d$에 유사성 점수 $H^eW_\\alpha$를 행렬곱해주면, 디코더 hidden state의 각 구성 요소에 가중치를 반영할 수 있게 된다. 이렇게 세 가지 요소가 모두 곱해진 $H^eW_a{h_t}^d$는 attention score로 기능하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TpRvQAt6ElZ"
   },
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxkfyOMk6F16"
   },
   "source": [
    "$e_t$ 행렬에서 $i$번째 행은 attention score $e_{t,i}$가 되며, 이때 $e_{t,i}$는 $i$번째 인코더 hidden state와  ${h_t}^d$(=현재 디코더 hidden descriptor)가 서로 관련된 정도 즉 유사도를 표현해주게 된다. 계산 과정을 풀어보자면, 예컨대 $e_{t,1}$을 계산하려면 ${h_1}^e$와 W를 행렬곱한 값에다 D차원의 ${h_t}^d$를 하나씩 곱해줌으로써 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_t = np.random.randn(1, N ,D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. * 사용 뒤 sum (교재에 제시된 방식)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "3qJr_-286R7T",
    "outputId": "1396f260-f687-4925-b062-61481b7fdca6"
   },
   "outputs": [],
   "source": [
    "## e_t = {H^eW_a * {h_t}^d}.sum(axis=-1) \n",
    "## 코드로 적용하면?\n",
    "e_t = (precomputed_encoder_score_vectors * h_t).sum(axis = -1)\n",
    "# (T, N, D) * (1, N, D) -> (T, N, D).sum(axis=-1) -> (T, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 코드?\n",
    "e_t = np.einsum(\"TND, ND -> TN\", precomputed_encoder_score_vectors, h_t.squeeze(0))\n",
    "e_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 18)\n",
      "(32, 18)\n"
     ]
    }
   ],
   "source": [
    "print(h_t.shape)\n",
    "print(h_t.squeeze(0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zQuuAKm6YrV"
   },
   "source": [
    ">잠깐! 3차원 데이터를 다룰 때 데이터의 방향에는 x축 방향, y축 방향, z축 방향이 있다. numpy에서 axis=0는 x축 방향, axis=1는 y축 방향, axis=2은 z축 방향을 가리키며, 3차원 데이터에서 axis=-1는 곧 axis=2 즉 z축 방향을 의미한다. 따라서, 위 수식에서 ‘sum(axis=-1)’란, {*}라는 3차원 데이터에서 z축 방향으로 같은 위치에 놓인 value끼리 합해준다는 것을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySMUL3yC6cvP"
   },
   "source": [
    "이 attention scores 벡터를 사용하면, 디코더 hidden descriptor에 대한 attention weights를 계산할 수 있다. 만약 어떠한 인코더 state의 weight가 특히 높다면, \"현재 디코더 step의 경우, 다른 hidden states보다 해당 인코더 state에 특히 주목해야 한다\"는 정보로 이해할 수 있다. 같은 방식으로, 낮은 가중치값에 대해서는 \"현재 디코더 단계의 경우, 이 특정한 인코더 hidden state에 포함된 정보를 대부분 무시해도 된다.\"라는 의미로 이해하면 된다.\n",
    "\n",
    "이러한 ① → ②의 방식으로 학습 과정을 반복하면서, 모델은 각 디코더 hidden descriptor에게 각 인코더 hidden state의 중요성을 부여하는 방법을 학습하게 된다. 다시 말해, 모든 인코더 hidden states 가운데 특정 i번째 디코더 hidden state와 가장 ‘관련 있는’ 인코더 hidden state가 무엇인지 더 잘 판단할 수 있도록, i번째 가중치 벡터가 조율된다고 요약할 수 있다. 만약 특정 인코더 hidden state가 어떤 디코더 hidden state(D차원)에 대해 D개의 항 전반에 걸쳐 높은 관련도를 가지고 있다면, 가중치 벡터는 이러한 정보를 학습한 후 attention score을 통해 증폭시킬 것이다. 이렇게 학습을 통해 개선된 가중치 행렬을 통해 모델은 인코더의 hidden descriptors와 디코더의 hidden descriptors 사이의 더 복잡한 문맥적 관계를 파악할 수 있게 된다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jl-Nh0e96fKI"
   },
   "source": [
    "### **2.3 attention weight 계산하기**\n",
    "\n",
    "실제로 문맥 벡터를 구성하는 값은 attention score가 아니라 attention score를 확률값 형태로 표현한 attention weight이다. 이때 필요한 것은 소프트맥스 함수이다. 우리는 소프트 맥스 함수에 (현재 디코더 hidden descriptor에 대한) attention score을 입력하여, 전체에 대한 확률값(0과 1사이의 값) 형태인 attention weight를 구할 수 있다.\n",
    "\n",
    "$\\alpha_t=softmax(e_t)=\\frac{exp(e_t)}{\\sum_{i=1}^{T}exp(e_{t,i})}=\\frac{exp(e_t)}{exp(e_{t,1})+exp(e_{t,2})+\\cdots+exp(e_{t,T})}$  \n",
    "\n",
    "편리하게도, MyGrad에는 소프트맥스 기능이 내장되어 있다! 단, 소프트맥스를 attention score 행렬에 취할 때, score에 해당되는 축에서 이루어져야 함을 명심하자.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "GmJYnDic6rAi",
    "outputId": "be83044e-024d-4044-b960-91208951c84f"
   },
   "outputs": [],
   "source": [
    "## attention weight = softmax(attention score) \n",
    "## 코드로 적용하면?\n",
    "\n",
    "# softmax((T, N), axis=0) -> (T, N)\n",
    "a_t = mg. nnet.softmax(e_t, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeP5E6AE6tft"
   },
   "source": [
    "### **2.4 문맥벡터 계산하기**\n",
    "마지막으로, 우리는 모든 인코더 hidden states의 정보를 '요약'해주는 문맥 벡터 $c_t$를 계산하기 위해 방금 계산한 attention weights를 사용할 것이다. 아래 식에서 알 수 있듯 attention weights $\\alpha_j$는 각 인코더 hidden state ${h_j}^e$의 계수가 되며, $\\alpha_j$와 ${h_j}^e$에 대한 가중합이 문맥벡터로 기능하게 된다.  \n",
    "\n",
    "$c_t=\\sum_{j=1}^{T}\\alpha_j{h_j}^e=\\alpha_1({h_1}^e)+\\alpha_2({h_2}^e)+\\cdots+\\alpha_T({h_T}^e)$  \n",
    "\n",
    "관련성이 상대적으로 적은 인코더 hidden states에는 낮은 attention weights가 주어지게 된다. 따라서, 문맥 벡터는 주로 가장 관련성이 높은 hidden states의 정보를 포함하게 될 것이다. 이를 구현할 때, 1번째 축(N)이 배치 차원 즉 배치 내 시퀀스의 개수를 나타내므로 이에 유의하여 시퀀스 길이를 나타내는 0번째 축(T)을 따라 총합을 계산해야 함을 기억하라.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. * 사용 뒤 sum (교재에 제시된 방식)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 32, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_t[..., None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "oGtP3ZnR6yig",
    "outputId": "b6b07887-ad4c-43a2-ebf3-7158b89507c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 32, 18)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## context vector = {\\alpha_j * {h_j}^e}의 가중합\n",
    "## 코드로 적용하면?\n",
    "c_t = (a_t[..., None] * encoder_hidden_states).sum(axis=0, keepdims = True)\n",
    "# (T, N, 1) * (T, N, D) -> (T, N, D).sum(axis=0, keepdims=True) -> (1, N, D)\n",
    "c_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 32, 18)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 코드?\n",
    "\n",
    "c_t = mg.einsum(\"TN,TND->ND\", a_t, encoder_hidden_states)\n",
    "c_t = np.expand_dims(c_t, 0)\n",
    "c_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 32, 18)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_t = mg.einsum(\"TN,TND->ND\", a_t, encoder_hidden_states)\n",
    "c_t = c_t[None, ...]\n",
    "c_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2ohgJx263OG"
   },
   "source": [
    "### **2.5 문맥벡터를 디코더에 넘겨주기**\n",
    "\n",
    "지금까지 문맥벡터 과정을 훌륭하게 이해해보았다. 이제는 문맥 벡터를 어떻게 다루어야 할지 고민해야 한다. 어떤 방식을 통해 디코더에 문맥 벡터와 디코더의 output을 둘 다 입력해줄 수 있을까? 기계학습 연구원들이 해온 방식에 여러 가지가 있는데, 그 중 우리가 취해볼 접근법은, 문맥 벡터 $c_t$를 일반적인 디코더 단계의 output $y_t$과 연결하는(concatenate) 방식이다. 즉, 우리는 디코더 단계에 대한 계산을 $s_t$와 ${h_t}^d$를 사용하여 평소대로 진행하고 $y_t$와 ${h_t}^d$를 얻어낼 것이다. 우리는 $y_t$ 값을 저장한 후, 다음 디코더의 input $s_{t+1}$에는 $y_t$와 문맥 벡터 $c_t$를 이어 붙인 값을 넣어줄 것이다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 32, 30)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 문맥벡터를 디코더에 넘겨주기 = 문맥벡터와 디코더 output을 연결\n",
    "## 코드로 적용하면?\n",
    "\n",
    "K = 12 # dim_output in decoder(RNN)\n",
    "y_t = np.random.randn(1, N, K)\n",
    "\n",
    "y_and_c = mg.concatenate([y_t, c_t], axis = -1) # (1, N, K), (1, N, D) -> (1,N, K+D)\n",
    "y_and_c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56lLu1Ho676X"
   },
   "source": [
    "우리는 output 벡터의 차원을 변경하기 때문에, dense layer를 사용하여 output 차원을 수정하고 최종 분류 점수를 재계산해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "id": "ncFKCtv-69cQ",
    "outputId": "8addecf3-1d5a-41db-c316-fd92fbb55635"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 32, 12)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 코드?\n",
    "post_concat_dense = dense(K+D, K)\n",
    "y_t = post_concat_dense(y_and_c)\n",
    "y_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBgRj2fO6-gu"
   },
   "source": [
    "이게 전부이다! 마지막 dense layer는 일종의 인터프리터 역할을 하는데, 문맥 벡터에서 전달되는 정보와 디코더 output의 정보를 통합해준다. 따라서 각 디코더 단계에 대한 최종 ouput에는 가장 중요한 인코더 hidden states로부터 직접 가져온 정보가 포함된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ST9vRU9C7ANa"
   },
   "source": [
    "## **3. 코드로 seq2seq with attention 구현하기** (HW)\n",
    "\n",
    "이제는, 이 모든 것들을 실행에 옮겨보자. 다행히 이전에 만들어두었던 RNN 클래스를 활용하면, 우리가 현재 필요로하는 텐서, 즉 모든 hidden states를 포함하는 텐서를 반환할 수 있다. 덕분에 우리는 $W_\\alpha$에 대한 MyNN  <dense>만 잘 정의해주면, attention scores를 계산할 수 있게 되었다. 이때 $H^eW_a$에 대한 계산은, 인코더의 output을 받은 직후, 어떤 다른 decoder 단계가 진행되기 전에 이루어져야 함을 명심하자.\n",
    "\n",
    "아래 클래스에서 위에서 설명한 attention mechanism을 적용하여 이전의 seq2seq 모델을 강화해보자. 각 디코더 단계에서는 계산된 attention weights를 저장하라. __call__에서는 예측 점수 및 attention weights에 대한 행렬(T,T,N) 을 모두(두 개) 반환하라. \n",
    "\n",
    "- 이때 (T,T,N)은 데이터를 이루는 시퀀스의 개수이며, 앞의 T,T는 데이터에 대한 길이를 의미한다. 차원이 2번 필요한 것은 각 토큰(T)별로 어떤 토큰(T)이 연관이 많은지 따져야 하기 때문이다. 구체적으로 보면 에서 첫 번째 T는 각 디코더 토큰이 인코더의 어떤 토큰과 관련 있는지(=attention weights)를 나타내주고, 두 번째 T는 디코더 토큰의 개수를 나타낸다. 둘의 의미를 혼동하지 않도록 하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "3fwehwqx7G1N"
   },
   "outputs": [],
   "source": [
    "class Attentionseq2seq:\n",
    "    def __init__(self, dim_input, dim_recurrent, dim_output):\n",
    "        \"\"\" Initializes all RNN layers needed for seq2seq\n",
    "        \n",
    "        매개변수\n",
    "        ----------\n",
    "        dim_input: int \n",
    "            RNN을 지나가는 데이터의 차원 (C)\n",
    "        \n",
    "        dim_recurrent: int\n",
    "            RNN에 있는 hidden state의 차원 (D)\n",
    "        \n",
    "        dim_output: int\n",
    "            RNN의 output의 차원 (K)\n",
    "        \n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        이 특정 문제에 대해, input의 차원과 output의 차원은 동일(C = K).\n",
    "        그러나 이것이 일반적인 경우는 아님.\n",
    "        \"\"\"\n",
    "        # (이전에 했던 것처럼) 두 개의 RNNs - 인코더와 디코더 -를 생성(인스턴스화)\n",
    "        #\n",
    "        # 이후, 두 개의  MyNN dense layer 생성:\n",
    "        # - 하나는 (D, D) 행렬 W_alpha에 해당되는 것 (bias는 없음),\n",
    "        # - 하나는 y_t 벡터와 c_t 벡터가 concat된 것 (dim K+D)\n",
    "        #   적절한 output dimension (dim K)을 갖춘 벡터.\n",
    "        #\n",
    "        # 두 개의 dense layers 모두, glorot_normal weight initializer 사용\n",
    "        # 여기에 코드 작성\n",
    "        self.encoder = RNN(dim_input, dim_recurrent, dim_output)\n",
    "        self.decoder = RNN(dim_input, dim_recurrent, dim_output)\n",
    "\n",
    "        # W_alpha = dense(D, D)\n",
    "        # precomputed_encoder_score_vectors = W_alpha(encoder_hidden_states)\n",
    "        # (T, N, D) @ (D, D) -> (T, N, D)\n",
    "\n",
    "        self.W_alpha = dense(dim_recurrent, dim_recurrent, weight_initializer=glorot_normal, bias=False)\n",
    "        \n",
    "        ## 코드?\n",
    "        # K = 12 # dim_output in decoder(RNN)\n",
    "        # y_t = np.random.randn(1, N, K)\n",
    "\n",
    "        # y_and_c = mg.concatenate([y_t, c_t], axis = -1) # (1, N, K), (1, N, D) -> (1,N, K+D)\n",
    "        # post_concat_dense = dense(K+D, K)\n",
    "        # y_t = post_concat_dense(y_and_c)\n",
    "        # y_t.shape\n",
    "        \n",
    "        self.post_concat_dense = dense(dim_output + dim_recurrent, dim_output, weight_initializer=glorot_normal)\n",
    "        \n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\" seq2seq에 대한 완전한 순방향 패스(full forward pass)를 수행.\n",
    "        \n",
    "        매개변수\n",
    "        ----------\n",
    "        x: Union[numpy.ndarray, mygrad.Tensor], shape=(T, N, C)\n",
    "            batch에 있는 각 시퀀스에 대한 원-핫 인코딩\n",
    "        \n",
    "        반환 값\n",
    "        -------\n",
    "        y: mygrad.Tensor, shape=(T, N, K)\n",
    "            각 디코더 step의 output으로부터 산출된 최종 분류 점수\n",
    "        a_ij: numpy.array, shape=(T, T, N)\n",
    "            sequences의 batch에 대한 attention weights. The 0-th\n",
    "            0차원은 각 디코더 step에서 계산된 attention weights에 상응함\n",
    "        \"\"\"\n",
    "        # (1) 인코딩 및 디코더 setup을 `seq2seq` 모델에서와 동일하게 수행:\n",
    "        #\n",
    "        # - 인코더 hidden states `enc_h`를 받음,\n",
    "        # - 첫 번째 디코더 hidden state가 마지막 인코더 hidden state가 되도록 설정\n",
    "        # - 리스트 `y`를 생성하여 디코더 outputs를 저장\n",
    "        # - <START> 토큰 원-핫 인코딩을 첫 번째 디코더 input으로 초기설정\n",
    "        # 여기에 코드 작성 (HW)\n",
    "        T, N, C = x.shape\n",
    "        y_t, enc_h = self.encoder(x)\n",
    "        h_t = enc_h\n",
    "        y = []\n",
    "        \n",
    "        y_t = one_hot_encode_prediction(y_t)\n",
    "\n",
    "        # (2) W_alpha에 상응하는 dense layer를 `enc_h`에 적용하여\n",
    "        # attention scores의 일부를 미리 계산\n",
    "        # 여기에 코드 작성 (HW)\n",
    "        # self.W_alpha = dense(dim_recurrent, dim_recurrent, weight_initializer=glorot_normal, bias=False)\n",
    "        precomputed_encoder_score_vectors = self.W_alpha(enc_h)\n",
    "\n",
    "        # (3) 아래 코드는 `a_ij` 리스트를 생성\n",
    "        # 리스트에 각 디코더 step에서의 attention weights를 저장\n",
    "        # 코드 다 주어짐\n",
    "        a_ij = []\n",
    "        \n",
    "        for _ in range(T):\n",
    "            # (4) `e_t`:현재 디코더 hidden state에 대한 attention scores 계산\n",
    "            # 사용 수식: 미리 계산된 W_alpha * H^e. \n",
    "            # 여기에 코드 작성 (HW)            \n",
    "            \n",
    "            # e_t = (precomputed_encoder_score_vectors * h_t).sum(axis = -1)\n",
    "            # (T, N, D) * (1, N, D) -> (T, N, D).sum(axis=-1) -> (T, N)\n",
    "            \n",
    "            e_t = (precomputed_encoder_score_vectors * h_t).sum(axis = -1)\n",
    "            \n",
    "            # (5) attention weights: attention scores에 소프트맥스 함수를 취한 것\n",
    "            # `T` 차원에 대해 softmax 함수를 취할 것을 명심하라.\n",
    "            # 결과를 변수 `a_t`에 저장.\n",
    "            # 여기에 코드 작성 (HW)\n",
    "            \n",
    "            a_t = softmax_crossentropy(e_t)\n",
    "            \n",
    "            # (6) 아래 코드는 현재 디코더 step에 대한 attention weights를 `a_ij` 리스트에 더해줌\n",
    "            # 새로운 축은 weights가 하나의 행렬로 연결(concatenate)될 수 있도록 만듦\n",
    "            # 코드 다 주어짐\n",
    "            a_ij.append(a_t.data[None]) # None : 차원 추가의 의미\n",
    "\n",
    "            # (7) 인코더 hidden states로부터 문맥 벡터 `c_t`를 계산하라 \n",
    "            # `enc_h` & the attention weights `a_t`.\n",
    "            # 여기에 코드 작성 (HW)\n",
    "            \n",
    "            c_t = mg.einsum(\"TN,TND->ND\", a_t, enc_h)\n",
    "            \n",
    "            # (8) basic seq2seq 모델에서처럼, \n",
    "            # 하나의 디코더 step를 수행하여 y_t와 h_t를 입력받아라\n",
    "            # 여기에 코드 작성 (HW)\n",
    "            \n",
    "            y_t, h_t = self.decoder(y_t, h_t)\n",
    "            \n",
    "            # (9) 마지막 축을 따라 디코더 output y_t와 문맥벡터 c_t를 연결(concatenate)하라            \n",
    "            # (1, N, K)와 (1, N, D)를 연결한 결과 (1, N, K+D)의 shape를 가짐\n",
    "            # 여기에 코드 작성 (HW)\n",
    "            \n",
    "            # y_and_c = mg.concatenate([y_t, c_t], axis = -1) # (1, N, K), (1, N, D) -> (1,N, K+D)\n",
    "            \n",
    "            y_and_c = mg.concatenate([y_t, c_t], axis = -1) # (1, N, K), (1, N, D) -> (1,N, K+D)\n",
    "            \n",
    "            # (10) dense layer를 활용하여\n",
    "            # 연결한(concatenated) 벡터를 -> 적절한 output 차원(K)의 벡터로 압축시켜라\n",
    "            # 리스트 `y`에 마지막 `y_t`를 추가하라\n",
    "            # 여기에 코드 작성 (HW)\n",
    "            \n",
    "            y_t = self.post_concat_dense(y_t)\n",
    "            y.append(y_t)\n",
    "\n",
    "            # (11) `one_hot_encode_prediction`를 사용하여 \n",
    "            # 디코더 output과 문맥벡터 `y_t`의 결합물을 기반으로\n",
    "            # 다음 디코더 input s_{t+1}을 찾아내라\n",
    "            # 여기에 코드 작성 (HW)\n",
    "            \n",
    "            y_t = np.argmax(one_hot_encode_prediction(y_and_c))\n",
    "\n",
    "        # (12) 0-th axis을 따라\n",
    "        # `y`에 저장된 y_t 텐서들을 모두 연결(concatenate)하라\n",
    "        # 여기에 코드 작성 (HW)\n",
    "\n",
    "        y = np.concatenate(y, axis=0)\n",
    "        \n",
    "        # (13) 아래 코드를 통해\n",
    "        # 각 디코더 step에 대해 계산된 T개의 attention vectors `a_t`를 연결할 수 있다\n",
    "        # 코드 다 주어짐\n",
    "        \n",
    "        a_ij = np.concatenate(a_ij, axis=0) # a_ij: shape-(T, T, N)\n",
    "\n",
    "        # (14) 위 docstring에 부합하는 적절한 텐서들과 배열들을 반환하라\n",
    "        # 여기에 코드 작성 (HW)\n",
    "        return y,  a_ij\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" 모델 내 모든 매개변수를 취하는 편리한 함수\n",
    "        이것은 `model.parameters`를 통해 속성(attribute)으로 액세스 가능\n",
    "        \n",
    "        반환 값\n",
    "        -------\n",
    "        Tuple[Tensor, ...]\n",
    "            우리 모델에 대한 모든 학습 가능한 매개변수를 포함하는 튜플\n",
    "        \"\"\"\n",
    "\n",
    "        # 여기에 코드 작성 (HW)\n",
    "        return self.encoder.parameters + self.decoder.parameters + self.W_alpha.parameters + self.post_concat_dense.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umtrjHRc7S00"
   },
   "source": [
    "여기에 noggin plot을 그려(인스턴스화) 모델의 손실과 정확성을 추적하라.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "CMDw3tO57Tjl",
    "outputId": "c96d0592-8e59-4dde-c557-42ea39bcf462"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "/* Put everything inside the global mpl namespace */\n/* global mpl */\nwindow.mpl = {};\n\nmpl.get_websocket_type = function () {\n    if (typeof WebSocket !== 'undefined') {\n        return WebSocket;\n    } else if (typeof MozWebSocket !== 'undefined') {\n        return MozWebSocket;\n    } else {\n        alert(\n            'Your browser does not have WebSocket support. ' +\n                'Please try Chrome, Safari or Firefox ≥ 6. ' +\n                'Firefox 4 and 5 are also supported but you ' +\n                'have to enable WebSockets in about:config.'\n        );\n    }\n};\n\nmpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n    this.id = figure_id;\n\n    this.ws = websocket;\n\n    this.supports_binary = this.ws.binaryType !== undefined;\n\n    if (!this.supports_binary) {\n        var warnings = document.getElementById('mpl-warnings');\n        if (warnings) {\n            warnings.style.display = 'block';\n            warnings.textContent =\n                'This browser does not support binary websocket messages. ' +\n                'Performance may be slow.';\n        }\n    }\n\n    this.imageObj = new Image();\n\n    this.context = undefined;\n    this.message = undefined;\n    this.canvas = undefined;\n    this.rubberband_canvas = undefined;\n    this.rubberband_context = undefined;\n    this.format_dropdown = undefined;\n\n    this.image_mode = 'full';\n\n    this.root = document.createElement('div');\n    this.root.setAttribute('style', 'display: inline-block');\n    this._root_extra_style(this.root);\n\n    parent_element.appendChild(this.root);\n\n    this._init_header(this);\n    this._init_canvas(this);\n    this._init_toolbar(this);\n\n    var fig = this;\n\n    this.waiting = false;\n\n    this.ws.onopen = function () {\n        fig.send_message('supports_binary', { value: fig.supports_binary });\n        fig.send_message('send_image_mode', {});\n        if (fig.ratio !== 1) {\n            fig.send_message('set_device_pixel_ratio', {\n                device_pixel_ratio: fig.ratio,\n            });\n        }\n        fig.send_message('refresh', {});\n    };\n\n    this.imageObj.onload = function () {\n        if (fig.image_mode === 'full') {\n            // Full images could contain transparency (where diff images\n            // almost always do), so we need to clear the canvas so that\n            // there is no ghosting.\n            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n        }\n        fig.context.drawImage(fig.imageObj, 0, 0);\n    };\n\n    this.imageObj.onunload = function () {\n        fig.ws.close();\n    };\n\n    this.ws.onmessage = this._make_on_message_function(this);\n\n    this.ondownload = ondownload;\n};\n\nmpl.figure.prototype._init_header = function () {\n    var titlebar = document.createElement('div');\n    titlebar.classList =\n        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n    var titletext = document.createElement('div');\n    titletext.classList = 'ui-dialog-title';\n    titletext.setAttribute(\n        'style',\n        'width: 100%; text-align: center; padding: 3px;'\n    );\n    titlebar.appendChild(titletext);\n    this.root.appendChild(titlebar);\n    this.header = titletext;\n};\n\nmpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n\nmpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n\nmpl.figure.prototype._init_canvas = function () {\n    var fig = this;\n\n    var canvas_div = (this.canvas_div = document.createElement('div'));\n    canvas_div.setAttribute('tabindex', '0');\n    canvas_div.setAttribute(\n        'style',\n        'border: 1px solid #ddd;' +\n            'box-sizing: content-box;' +\n            'clear: both;' +\n            'min-height: 1px;' +\n            'min-width: 1px;' +\n            'outline: 0;' +\n            'overflow: hidden;' +\n            'position: relative;' +\n            'resize: both;' +\n            'z-index: 2;'\n    );\n\n    function on_keyboard_event_closure(name) {\n        return function (event) {\n            return fig.key_event(event, name);\n        };\n    }\n\n    canvas_div.addEventListener(\n        'keydown',\n        on_keyboard_event_closure('key_press')\n    );\n    canvas_div.addEventListener(\n        'keyup',\n        on_keyboard_event_closure('key_release')\n    );\n\n    this._canvas_extra_style(canvas_div);\n    this.root.appendChild(canvas_div);\n\n    var canvas = (this.canvas = document.createElement('canvas'));\n    canvas.classList.add('mpl-canvas');\n    canvas.setAttribute(\n        'style',\n        'box-sizing: content-box;' +\n            'pointer-events: none;' +\n            'position: relative;' +\n            'z-index: 0;'\n    );\n\n    this.context = canvas.getContext('2d');\n\n    var backingStore =\n        this.context.backingStorePixelRatio ||\n        this.context.webkitBackingStorePixelRatio ||\n        this.context.mozBackingStorePixelRatio ||\n        this.context.msBackingStorePixelRatio ||\n        this.context.oBackingStorePixelRatio ||\n        this.context.backingStorePixelRatio ||\n        1;\n\n    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n\n    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n        'canvas'\n    ));\n    rubberband_canvas.setAttribute(\n        'style',\n        'box-sizing: content-box;' +\n            'left: 0;' +\n            'pointer-events: none;' +\n            'position: absolute;' +\n            'top: 0;' +\n            'z-index: 1;'\n    );\n\n    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n    if (this.ResizeObserver === undefined) {\n        if (window.ResizeObserver !== undefined) {\n            this.ResizeObserver = window.ResizeObserver;\n        } else {\n            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n            this.ResizeObserver = obs.ResizeObserver;\n        }\n    }\n\n    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n        var nentries = entries.length;\n        for (var i = 0; i < nentries; i++) {\n            var entry = entries[i];\n            var width, height;\n            if (entry.contentBoxSize) {\n                if (entry.contentBoxSize instanceof Array) {\n                    // Chrome 84 implements new version of spec.\n                    width = entry.contentBoxSize[0].inlineSize;\n                    height = entry.contentBoxSize[0].blockSize;\n                } else {\n                    // Firefox implements old version of spec.\n                    width = entry.contentBoxSize.inlineSize;\n                    height = entry.contentBoxSize.blockSize;\n                }\n            } else {\n                // Chrome <84 implements even older version of spec.\n                width = entry.contentRect.width;\n                height = entry.contentRect.height;\n            }\n\n            // Keep the size of the canvas and rubber band canvas in sync with\n            // the canvas container.\n            if (entry.devicePixelContentBoxSize) {\n                // Chrome 84 implements new version of spec.\n                canvas.setAttribute(\n                    'width',\n                    entry.devicePixelContentBoxSize[0].inlineSize\n                );\n                canvas.setAttribute(\n                    'height',\n                    entry.devicePixelContentBoxSize[0].blockSize\n                );\n            } else {\n                canvas.setAttribute('width', width * fig.ratio);\n                canvas.setAttribute('height', height * fig.ratio);\n            }\n            /* This rescales the canvas back to display pixels, so that it\n             * appears correct on HiDPI screens. */\n            canvas.style.width = width + 'px';\n            canvas.style.height = height + 'px';\n\n            rubberband_canvas.setAttribute('width', width);\n            rubberband_canvas.setAttribute('height', height);\n\n            // And update the size in Python. We ignore the initial 0/0 size\n            // that occurs as the element is placed into the DOM, which should\n            // otherwise not happen due to the minimum size styling.\n            if (fig.ws.readyState == 1 && width != 0 && height != 0) {\n                fig.request_resize(width, height);\n            }\n        }\n    });\n    this.resizeObserverInstance.observe(canvas_div);\n\n    function on_mouse_event_closure(name) {\n        /* User Agent sniffing is bad, but WebKit is busted:\n         * https://bugs.webkit.org/show_bug.cgi?id=144526\n         * https://bugs.webkit.org/show_bug.cgi?id=181818\n         * The worst that happens here is that they get an extra browser\n         * selection when dragging, if this check fails to catch them.\n         */\n        var UA = navigator.userAgent;\n        var isWebKit = /AppleWebKit/.test(UA) && !/Chrome/.test(UA);\n        if(isWebKit) {\n            return function (event) {\n                /* This prevents the web browser from automatically changing to\n                 * the text insertion cursor when the button is pressed. We\n                 * want to control all of the cursor setting manually through\n                 * the 'cursor' event from matplotlib */\n                event.preventDefault()\n                return fig.mouse_event(event, name);\n            };\n        } else {\n            return function (event) {\n                return fig.mouse_event(event, name);\n            };\n        }\n    }\n\n    canvas_div.addEventListener(\n        'mousedown',\n        on_mouse_event_closure('button_press')\n    );\n    canvas_div.addEventListener(\n        'mouseup',\n        on_mouse_event_closure('button_release')\n    );\n    canvas_div.addEventListener(\n        'dblclick',\n        on_mouse_event_closure('dblclick')\n    );\n    // Throttle sequential mouse events to 1 every 20ms.\n    canvas_div.addEventListener(\n        'mousemove',\n        on_mouse_event_closure('motion_notify')\n    );\n\n    canvas_div.addEventListener(\n        'mouseenter',\n        on_mouse_event_closure('figure_enter')\n    );\n    canvas_div.addEventListener(\n        'mouseleave',\n        on_mouse_event_closure('figure_leave')\n    );\n\n    canvas_div.addEventListener('wheel', function (event) {\n        if (event.deltaY < 0) {\n            event.step = 1;\n        } else {\n            event.step = -1;\n        }\n        on_mouse_event_closure('scroll')(event);\n    });\n\n    canvas_div.appendChild(canvas);\n    canvas_div.appendChild(rubberband_canvas);\n\n    this.rubberband_context = rubberband_canvas.getContext('2d');\n    this.rubberband_context.strokeStyle = '#000000';\n\n    this._resize_canvas = function (width, height, forward) {\n        if (forward) {\n            canvas_div.style.width = width + 'px';\n            canvas_div.style.height = height + 'px';\n        }\n    };\n\n    // Disable right mouse context menu.\n    canvas_div.addEventListener('contextmenu', function (_e) {\n        event.preventDefault();\n        return false;\n    });\n\n    function set_focus() {\n        canvas.focus();\n        canvas_div.focus();\n    }\n\n    window.setTimeout(set_focus, 100);\n};\n\nmpl.figure.prototype._init_toolbar = function () {\n    var fig = this;\n\n    var toolbar = document.createElement('div');\n    toolbar.classList = 'mpl-toolbar';\n    this.root.appendChild(toolbar);\n\n    function on_click_closure(name) {\n        return function (_event) {\n            return fig.toolbar_button_onclick(name);\n        };\n    }\n\n    function on_mouseover_closure(tooltip) {\n        return function (event) {\n            if (!event.currentTarget.disabled) {\n                return fig.toolbar_button_onmouseover(tooltip);\n            }\n        };\n    }\n\n    fig.buttons = {};\n    var buttonGroup = document.createElement('div');\n    buttonGroup.classList = 'mpl-button-group';\n    for (var toolbar_ind in mpl.toolbar_items) {\n        var name = mpl.toolbar_items[toolbar_ind][0];\n        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n        var image = mpl.toolbar_items[toolbar_ind][2];\n        var method_name = mpl.toolbar_items[toolbar_ind][3];\n\n        if (!name) {\n            /* Instead of a spacer, we start a new button group. */\n            if (buttonGroup.hasChildNodes()) {\n                toolbar.appendChild(buttonGroup);\n            }\n            buttonGroup = document.createElement('div');\n            buttonGroup.classList = 'mpl-button-group';\n            continue;\n        }\n\n        var button = (fig.buttons[name] = document.createElement('button'));\n        button.classList = 'mpl-widget';\n        button.setAttribute('role', 'button');\n        button.setAttribute('aria-disabled', 'false');\n        button.addEventListener('click', on_click_closure(method_name));\n        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n\n        var icon_img = document.createElement('img');\n        icon_img.src = '_images/' + image + '.png';\n        icon_img.srcset = '_images/' + image + '_large.png 2x';\n        icon_img.alt = tooltip;\n        button.appendChild(icon_img);\n\n        buttonGroup.appendChild(button);\n    }\n\n    if (buttonGroup.hasChildNodes()) {\n        toolbar.appendChild(buttonGroup);\n    }\n\n    var fmt_picker = document.createElement('select');\n    fmt_picker.classList = 'mpl-widget';\n    toolbar.appendChild(fmt_picker);\n    this.format_dropdown = fmt_picker;\n\n    for (var ind in mpl.extensions) {\n        var fmt = mpl.extensions[ind];\n        var option = document.createElement('option');\n        option.selected = fmt === mpl.default_extension;\n        option.innerHTML = fmt;\n        fmt_picker.appendChild(option);\n    }\n\n    var status_bar = document.createElement('span');\n    status_bar.classList = 'mpl-message';\n    toolbar.appendChild(status_bar);\n    this.message = status_bar;\n};\n\nmpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n    // which will in turn request a refresh of the image.\n    this.send_message('resize', { width: x_pixels, height: y_pixels });\n};\n\nmpl.figure.prototype.send_message = function (type, properties) {\n    properties['type'] = type;\n    properties['figure_id'] = this.id;\n    this.ws.send(JSON.stringify(properties));\n};\n\nmpl.figure.prototype.send_draw_message = function () {\n    if (!this.waiting) {\n        this.waiting = true;\n        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n    }\n};\n\nmpl.figure.prototype.handle_save = function (fig, _msg) {\n    var format_dropdown = fig.format_dropdown;\n    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n    fig.ondownload(fig, format);\n};\n\nmpl.figure.prototype.handle_resize = function (fig, msg) {\n    var size = msg['size'];\n    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n        fig._resize_canvas(size[0], size[1], msg['forward']);\n        fig.send_message('refresh', {});\n    }\n};\n\nmpl.figure.prototype.handle_rubberband = function (fig, msg) {\n    var x0 = msg['x0'] / fig.ratio;\n    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n    var x1 = msg['x1'] / fig.ratio;\n    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n    x0 = Math.floor(x0) + 0.5;\n    y0 = Math.floor(y0) + 0.5;\n    x1 = Math.floor(x1) + 0.5;\n    y1 = Math.floor(y1) + 0.5;\n    var min_x = Math.min(x0, x1);\n    var min_y = Math.min(y0, y1);\n    var width = Math.abs(x1 - x0);\n    var height = Math.abs(y1 - y0);\n\n    fig.rubberband_context.clearRect(\n        0,\n        0,\n        fig.canvas.width / fig.ratio,\n        fig.canvas.height / fig.ratio\n    );\n\n    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n};\n\nmpl.figure.prototype.handle_figure_label = function (fig, msg) {\n    // Updates the figure title.\n    fig.header.textContent = msg['label'];\n};\n\nmpl.figure.prototype.handle_cursor = function (fig, msg) {\n    fig.canvas_div.style.cursor = msg['cursor'];\n};\n\nmpl.figure.prototype.handle_message = function (fig, msg) {\n    fig.message.textContent = msg['message'];\n};\n\nmpl.figure.prototype.handle_draw = function (fig, _msg) {\n    // Request the server to send over a new figure.\n    fig.send_draw_message();\n};\n\nmpl.figure.prototype.handle_image_mode = function (fig, msg) {\n    fig.image_mode = msg['mode'];\n};\n\nmpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n    for (var key in msg) {\n        if (!(key in fig.buttons)) {\n            continue;\n        }\n        fig.buttons[key].disabled = !msg[key];\n        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n    }\n};\n\nmpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n    if (msg['mode'] === 'PAN') {\n        fig.buttons['Pan'].classList.add('active');\n        fig.buttons['Zoom'].classList.remove('active');\n    } else if (msg['mode'] === 'ZOOM') {\n        fig.buttons['Pan'].classList.remove('active');\n        fig.buttons['Zoom'].classList.add('active');\n    } else {\n        fig.buttons['Pan'].classList.remove('active');\n        fig.buttons['Zoom'].classList.remove('active');\n    }\n};\n\nmpl.figure.prototype.updated_canvas_event = function () {\n    // Called whenever the canvas gets updated.\n    this.send_message('ack', {});\n};\n\n// A function to construct a web socket function for onmessage handling.\n// Called in the figure constructor.\nmpl.figure.prototype._make_on_message_function = function (fig) {\n    return function socket_on_message(evt) {\n        if (evt.data instanceof Blob) {\n            var img = evt.data;\n            if (img.type !== 'image/png') {\n                /* FIXME: We get \"Resource interpreted as Image but\n                 * transferred with MIME type text/plain:\" errors on\n                 * Chrome.  But how to set the MIME type?  It doesn't seem\n                 * to be part of the websocket stream */\n                img.type = 'image/png';\n            }\n\n            /* Free the memory for the previous frames */\n            if (fig.imageObj.src) {\n                (window.URL || window.webkitURL).revokeObjectURL(\n                    fig.imageObj.src\n                );\n            }\n\n            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n                img\n            );\n            fig.updated_canvas_event();\n            fig.waiting = false;\n            return;\n        } else if (\n            typeof evt.data === 'string' &&\n            evt.data.slice(0, 21) === 'data:image/png;base64'\n        ) {\n            fig.imageObj.src = evt.data;\n            fig.updated_canvas_event();\n            fig.waiting = false;\n            return;\n        }\n\n        var msg = JSON.parse(evt.data);\n        var msg_type = msg['type'];\n\n        // Call the  \"handle_{type}\" callback, which takes\n        // the figure and JSON message as its only arguments.\n        try {\n            var callback = fig['handle_' + msg_type];\n        } catch (e) {\n            console.log(\n                \"No handler for the '\" + msg_type + \"' message type: \",\n                msg\n            );\n            return;\n        }\n\n        if (callback) {\n            try {\n                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n                callback(fig, msg);\n            } catch (e) {\n                console.log(\n                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n                    e,\n                    e.stack,\n                    msg\n                );\n            }\n        }\n    };\n};\n\nfunction getModifiers(event) {\n    var mods = [];\n    if (event.ctrlKey) {\n        mods.push('ctrl');\n    }\n    if (event.altKey) {\n        mods.push('alt');\n    }\n    if (event.shiftKey) {\n        mods.push('shift');\n    }\n    if (event.metaKey) {\n        mods.push('meta');\n    }\n    return mods;\n}\n\n/*\n * return a copy of an object with only non-object keys\n * we need this to avoid circular references\n * https://stackoverflow.com/a/24161582/3208463\n */\nfunction simpleKeys(original) {\n    return Object.keys(original).reduce(function (obj, key) {\n        if (typeof original[key] !== 'object') {\n            obj[key] = original[key];\n        }\n        return obj;\n    }, {});\n}\n\nmpl.figure.prototype.mouse_event = function (event, name) {\n    if (name === 'button_press') {\n        this.canvas.focus();\n        this.canvas_div.focus();\n    }\n\n    // from https://stackoverflow.com/q/1114465\n    var boundingRect = this.canvas.getBoundingClientRect();\n    var x = (event.clientX - boundingRect.left) * this.ratio;\n    var y = (event.clientY - boundingRect.top) * this.ratio;\n\n    this.send_message(name, {\n        x: x,\n        y: y,\n        button: event.button,\n        step: event.step,\n        modifiers: getModifiers(event),\n        guiEvent: simpleKeys(event),\n    });\n\n    return false;\n};\n\nmpl.figure.prototype._key_event_extra = function (_event, _name) {\n    // Handle any extra behaviour associated with a key event\n};\n\nmpl.figure.prototype.key_event = function (event, name) {\n    // Prevent repeat events\n    if (name === 'key_press') {\n        if (event.key === this._key) {\n            return;\n        } else {\n            this._key = event.key;\n        }\n    }\n    if (name === 'key_release') {\n        this._key = null;\n    }\n\n    var value = '';\n    if (event.ctrlKey && event.key !== 'Control') {\n        value += 'ctrl+';\n    }\n    else if (event.altKey && event.key !== 'Alt') {\n        value += 'alt+';\n    }\n    else if (event.shiftKey && event.key !== 'Shift') {\n        value += 'shift+';\n    }\n\n    value += 'k' + event.key;\n\n    this._key_event_extra(event, name);\n\n    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n    return false;\n};\n\nmpl.figure.prototype.toolbar_button_onclick = function (name) {\n    if (name === 'download') {\n        this.handle_save(this, null);\n    } else {\n        this.send_message('toolbar_button', { name: name });\n    }\n};\n\nmpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n    this.message.textContent = tooltip;\n};\n\n///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n// prettier-ignore\nvar _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\nmpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis\", \"fa fa-square-o\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o\", \"download\"]];\n\nmpl.extensions = [\"eps\", \"jpeg\", \"pgf\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\", \"webp\"];\n\nmpl.default_extension = \"png\";/* global mpl */\n\nvar comm_websocket_adapter = function (comm) {\n    // Create a \"websocket\"-like object which calls the given IPython comm\n    // object with the appropriate methods. Currently this is a non binary\n    // socket, so there is still some room for performance tuning.\n    var ws = {};\n\n    ws.binaryType = comm.kernel.ws.binaryType;\n    ws.readyState = comm.kernel.ws.readyState;\n    function updateReadyState(_event) {\n        if (comm.kernel.ws) {\n            ws.readyState = comm.kernel.ws.readyState;\n        } else {\n            ws.readyState = 3; // Closed state.\n        }\n    }\n    comm.kernel.ws.addEventListener('open', updateReadyState);\n    comm.kernel.ws.addEventListener('close', updateReadyState);\n    comm.kernel.ws.addEventListener('error', updateReadyState);\n\n    ws.close = function () {\n        comm.close();\n    };\n    ws.send = function (m) {\n        //console.log('sending', m);\n        comm.send(m);\n    };\n    // Register the callback with on_msg.\n    comm.on_msg(function (msg) {\n        //console.log('receiving', msg['content']['data'], msg);\n        var data = msg['content']['data'];\n        if (data['blob'] !== undefined) {\n            data = {\n                data: new Blob(msg['buffers'], { type: data['blob'] }),\n            };\n        }\n        // Pass the mpl event to the overridden (by mpl) onmessage function.\n        ws.onmessage(data);\n    });\n    return ws;\n};\n\nmpl.mpl_figure_comm = function (comm, msg) {\n    // This is the function which gets called when the mpl process\n    // starts-up an IPython Comm through the \"matplotlib\" channel.\n\n    var id = msg.content.data.id;\n    // Get hold of the div created by the display call when the Comm\n    // socket was opened in Python.\n    var element = document.getElementById(id);\n    var ws_proxy = comm_websocket_adapter(comm);\n\n    function ondownload(figure, _format) {\n        window.open(figure.canvas.toDataURL());\n    }\n\n    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n\n    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n    // web socket which is closed, not our websocket->open comm proxy.\n    ws_proxy.onopen();\n\n    fig.parent_element = element;\n    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n    if (!fig.cell_info) {\n        console.error('Failed to find cell for figure', id, fig);\n        return;\n    }\n    fig.cell_info[0].output_area.element.on(\n        'cleared',\n        { fig: fig },\n        fig._remove_fig_handler\n    );\n};\n\nmpl.figure.prototype.handle_close = function (fig, msg) {\n    var width = fig.canvas.width / fig.ratio;\n    fig.cell_info[0].output_area.element.off(\n        'cleared',\n        fig._remove_fig_handler\n    );\n    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n\n    // Update the output cell to use the data from the current canvas.\n    fig.push_to_output();\n    var dataURL = fig.canvas.toDataURL();\n    // Re-enable the keyboard manager in IPython - without this line, in FF,\n    // the notebook keyboard shortcuts fail.\n    IPython.keyboard_manager.enable();\n    fig.parent_element.innerHTML =\n        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n    fig.close_ws(fig, msg);\n};\n\nmpl.figure.prototype.close_ws = function (fig, msg) {\n    fig.send_message('closing', msg);\n    // fig.ws.close()\n};\n\nmpl.figure.prototype.push_to_output = function (_remove_interactive) {\n    // Turn the data on the canvas into data in the output cell.\n    var width = this.canvas.width / this.ratio;\n    var dataURL = this.canvas.toDataURL();\n    this.cell_info[1]['text/html'] =\n        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n};\n\nmpl.figure.prototype.updated_canvas_event = function () {\n    // Tell IPython that the notebook contents must change.\n    IPython.notebook.set_dirty(true);\n    this.send_message('ack', {});\n    var fig = this;\n    // Wait a second, then push the new image to the DOM so\n    // that it is saved nicely (might be nice to debounce this).\n    setTimeout(function () {\n        fig.push_to_output();\n    }, 1000);\n};\n\nmpl.figure.prototype._init_toolbar = function () {\n    var fig = this;\n\n    var toolbar = document.createElement('div');\n    toolbar.classList = 'btn-toolbar';\n    this.root.appendChild(toolbar);\n\n    function on_click_closure(name) {\n        return function (_event) {\n            return fig.toolbar_button_onclick(name);\n        };\n    }\n\n    function on_mouseover_closure(tooltip) {\n        return function (event) {\n            if (!event.currentTarget.disabled) {\n                return fig.toolbar_button_onmouseover(tooltip);\n            }\n        };\n    }\n\n    fig.buttons = {};\n    var buttonGroup = document.createElement('div');\n    buttonGroup.classList = 'btn-group';\n    var button;\n    for (var toolbar_ind in mpl.toolbar_items) {\n        var name = mpl.toolbar_items[toolbar_ind][0];\n        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n        var image = mpl.toolbar_items[toolbar_ind][2];\n        var method_name = mpl.toolbar_items[toolbar_ind][3];\n\n        if (!name) {\n            /* Instead of a spacer, we start a new button group. */\n            if (buttonGroup.hasChildNodes()) {\n                toolbar.appendChild(buttonGroup);\n            }\n            buttonGroup = document.createElement('div');\n            buttonGroup.classList = 'btn-group';\n            continue;\n        }\n\n        button = fig.buttons[name] = document.createElement('button');\n        button.classList = 'btn btn-default';\n        button.href = '#';\n        button.title = name;\n        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n        button.addEventListener('click', on_click_closure(method_name));\n        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n        buttonGroup.appendChild(button);\n    }\n\n    if (buttonGroup.hasChildNodes()) {\n        toolbar.appendChild(buttonGroup);\n    }\n\n    // Add the status bar.\n    var status_bar = document.createElement('span');\n    status_bar.classList = 'mpl-message pull-right';\n    toolbar.appendChild(status_bar);\n    this.message = status_bar;\n\n    // Add the close button to the window.\n    var buttongrp = document.createElement('div');\n    buttongrp.classList = 'btn-group inline pull-right';\n    button = document.createElement('button');\n    button.classList = 'btn btn-mini btn-primary';\n    button.href = '#';\n    button.title = 'Stop Interaction';\n    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n    button.addEventListener('click', function (_evt) {\n        fig.handle_close(fig, {});\n    });\n    button.addEventListener(\n        'mouseover',\n        on_mouseover_closure('Stop Interaction')\n    );\n    buttongrp.appendChild(button);\n    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n};\n\nmpl.figure.prototype._remove_fig_handler = function (event) {\n    var fig = event.data.fig;\n    if (event.target !== this) {\n        // Ignore bubbled events from children.\n        return;\n    }\n    fig.close_ws(fig, {});\n};\n\nmpl.figure.prototype._root_extra_style = function (el) {\n    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n};\n\nmpl.figure.prototype._canvas_extra_style = function (el) {\n    // this is important to make the div 'focusable\n    el.setAttribute('tabindex', 0);\n    // reach out to IPython and tell the keyboard manager to turn it's self\n    // off when our div gets focus\n\n    // location in version 3\n    if (IPython.notebook.keyboard_manager) {\n        IPython.notebook.keyboard_manager.register_events(el);\n    } else {\n        // location in version 2\n        IPython.keyboard_manager.register_events(el);\n    }\n};\n\nmpl.figure.prototype._key_event_extra = function (event, _name) {\n    // Check for shift+enter\n    if (event.shiftKey && event.which === 13) {\n        this.canvas_div.blur();\n        // select the cell after this one\n        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n        IPython.notebook.select(index + 1);\n    }\n};\n\nmpl.figure.prototype.handle_save = function (fig, _msg) {\n    fig.ondownload(fig, null);\n};\n\nmpl.find_output_cell = function (html_output) {\n    // Return the cell and output element which can be found *uniquely* in the notebook.\n    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n    // IPython event is triggered only after the cells have been serialised, which for\n    // our purposes (turning an active figure into a static one), is too late.\n    var cells = IPython.notebook.get_cells();\n    var ncells = cells.length;\n    for (var i = 0; i < ncells; i++) {\n        var cell = cells[i];\n        if (cell.cell_type === 'code') {\n            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n                var data = cell.output_area.outputs[j];\n                if (data.data) {\n                    // IPython >= 3 moved mimebundle to data attribute of output\n                    data = data.data;\n                }\n                if (data['text/html'] === html_output) {\n                    return [cell, data, j];\n                }\n            }\n        }\n    }\n};\n\n// Register the function which deals with the matplotlib target/channel.\n// The kernel may be null if the page has been refreshed.\nif (IPython.notebook.kernel !== null) {\n    IPython.notebook.kernel.comm_manager.register_target(\n        'matplotlib',\n        mpl.mpl_figure_comm\n    );\n}\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id='f0ae8943-a925-47ee-ae88-6c06b894939e'></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "from noggin import create_plot\n",
    "plotter, fig, ax = create_plot([\"loss\", \"accuracy\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxZoj9JW7V4S"
   },
   "source": [
    "Attentionseq2seq 모델과 Adam 옵티마이저를 정의하라. 모델의 경우 dim_recurrent가 25인 것이 좋다. 옵티마이저의 경우 매개변수를 default로 지정하는 것이 시작하는 단계에서 적합할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW\n",
    "dim_input과 dim_output은 얼마가 되어야 할까? dim_recurrent처럼 아무 값이나 설정할 수 있는 건 아니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ak7hd_UQ7W6M"
   },
   "outputs": [],
   "source": [
    "# 여기에 코드 작성 (HW)\n",
    "model = Attentionseq2seq(dim_input=12, dim_recurrent=25, dim_output=12)\n",
    "optimizer = Adam(model.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u48vw6UB7YOk"
   },
   "source": [
    "아래에 학습 loop를 작성하라. 필요에 따라 inputs를 softmax_crossentropy로 reshape해야 함을 기억하라. 이때 배치 크기는 100으로 지정하고, 시퀀스 길이는 1에서 20 사이가 되도록 하라. 모델을 8000회 반복하여 학습시켜라. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "TFZKIGp37Yt0",
    "outputId": "1ee8e9cf-536d-4f9d-e7dd-a71e1945066d"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "softmax_crossentropy() missing 1 required positional argument: 'y_true'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m8000\u001b[39m):\n\u001b[0;32m      5\u001b[0m     x, target, sequence \u001b[39m=\u001b[39m generate_batch(batch_size\u001b[39m=\u001b[39mbatch_size)\n\u001b[1;32m----> 7\u001b[0m     output, _ \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m      9\u001b[0m     loss \u001b[39m=\u001b[39m softmax_crossentropy(output\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m12\u001b[39m), target\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m     10\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "Cell \u001b[1;32mIn[27], line 109\u001b[0m, in \u001b[0;36mAttentionseq2seq.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    102\u001b[0m e_t \u001b[39m=\u001b[39m (precomputed_encoder_score_vectors \u001b[39m*\u001b[39m h_t)\u001b[39m.\u001b[39msum(axis \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    104\u001b[0m \u001b[39m# (5) attention weights: attention scores에 소프트맥스 함수를 취한 것\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m# `T` 차원에 대해 softmax 함수를 취할 것을 명심하라.\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[39m# 결과를 변수 `a_t`에 저장.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[39m# 여기에 코드 작성 (HW)\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m a_t \u001b[39m=\u001b[39m softmax_crossentropy(e_t)\n\u001b[0;32m    111\u001b[0m \u001b[39m# (6) 아래 코드는 현재 디코더 step에 대한 attention weights를 `a_ij` 리스트에 더해줌\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[39m# 새로운 축은 weights가 하나의 행렬로 연결(concatenate)될 수 있도록 만듦\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[39m# 코드 다 주어짐\u001b[39;00m\n\u001b[0;32m    114\u001b[0m a_ij\u001b[39m.\u001b[39mappend(a_t\u001b[39m.\u001b[39mdata[\u001b[39mNone\u001b[39;00m]) \u001b[39m# None : 차원 추가의 의미\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: softmax_crossentropy() missing 1 required positional argument: 'y_true'"
     ]
    }
   ],
   "source": [
    "# 여기에 코드 작성\n",
    "batch_size= 100\n",
    "\n",
    "for k in range(8000):\n",
    "    x, target, sequence = generate_batch(batch_size=batch_size)\n",
    "\n",
    "    output, _ = model(x)\n",
    "\n",
    "    loss = softmax_crossentropy(output.reshape(-1, 12), target.reshape(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    acc = np.mean(np.argmax(output, axis=-1) == target)\n",
    "\n",
    "    log = {\"loss\":loss.item(), \"accuracy\":acc}\n",
    "    plotter.set_train_batch({\"loss\":loss.item(), \"accuracy\":acc}, batch_size=batch_size)\n",
    "    \n",
    "\n",
    "    if k % 500 == 0 and k > 0:\n",
    "        plotter.set_train_epoch()\n",
    "\n",
    "plotter.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCHgijKh7aVI"
   },
   "source": [
    "다음을 실행하여 모형의 정확성을 평가하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpFPOsdl7b8R"
   },
   "outputs": [],
   "source": [
    "length_total = defaultdict(int)\n",
    "length_correct = defaultdict(int)\n",
    "\n",
    "with mg.no_autodiff:\n",
    "    for i in range(50000):\n",
    "        if i % 5000 == 0:\n",
    "            print(f\"i = {i}\")\n",
    "        x, target, sequence = generate_batch(1, 20, 1)\n",
    "\n",
    "        output, _ = model(x)\n",
    "    \n",
    "        length_total[sequence.size] += 1\n",
    "        if np.all(np.argmax(output, axis=-1) == target):\n",
    "            length_correct[sequence.size] += 1\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x, y = [], []\n",
    "for i in range(1, 20):\n",
    "    x.append(i)\n",
    "    y.append(length_correct[i] / length_total[i])\n",
    "ax.plot(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKcSgZim7dh3"
   },
   "source": [
    "이제 우리는 학습한 모든 시퀀스 길이에 대해 거의 완벽에 가까운 정확성을 확인할 수 있다. (충분히 오래 학습한 경우, 모델은 이 문제를 완전히 마스터할 수 있다).\n",
    "\n",
    "마무리 단계로, 단일 sequence에 대해 계산된 attention weights를 살펴보자. 아래 셀을 실행하면, 선택한 시퀀스 길이에 대한 attention weights를 시각화할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y0x-t36k7gF4"
   },
   "outputs": [],
   "source": [
    "seq_len = 15\n",
    "\n",
    "x, target, sequence = generate_batch(seq_len, seq_len, 1)\n",
    "y, a_ij = model(x)\n",
    "y = np.argmax(y, axis=-1).squeeze() # determine decoder inputs\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_yticks(range(y.size))\n",
    "ax.set_yticklabels([\"<S>\"] + [x for x in y[:-1]])\n",
    "ax.set_ylabel(\"Input to Decoder\")\n",
    "\n",
    "ax.xaxis.tick_top()\n",
    "ax.xaxis.set_label_position('top') \n",
    "\n",
    "ax.set_xticks(range(target.size))\n",
    "ax.set_xticklabels([x for x in sequence.squeeze()] + [\"<E>\"])\n",
    "ax.set_xlabel(\"Original Sequence\")\n",
    "\n",
    "ax.imshow(a_ij.squeeze());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXAORxA67gtb"
   },
   "source": [
    "이런 결과가 나올 것이라 예상하였는가? 우리가 해결하려는 문제를 고려할 때, 직관적으로 이해가 잘 되는가?\n",
    "\n",
    "사실상 딥러닝 분야에서 attention의 도입됨으로써 딥러닝을 이해하는 데 중요한 진전을 이루어낼 수 있었다. 실제로 attention은 매우 강력한 도구이기 때문에, 마지막 노트에서는 어떻게 우리가 attention만을 활용하여 언어 모델을 구성할 수 있는지 알아보도록 하자.  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP_실습11_Modified_Ver 1.0.ipynb",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "5fc68915adec3a51a85e3da5064832ada1f8b59a6042e0d39226d1098183b0ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
