{"cells":[{"cell_type":"markdown","metadata":{"id":"QPnOdOwb-SUh"},"source":["# **model.ipynb**\n","\n","중간 미션 task는 주어진 입력 문장을 긍정/부정으로 이진분류하는 모델을 만드는 것이다. 이진분류를 위한 모델을 생성하기 위해서, 우리는 사전 훈련된 Bert 모델을 가지고 온 뒤 네이버 영화 리뷰 데이터셋을 활용하여 해당 모델을 긍정/부정의 이진분류를 수행하는 모델로 학습시켜야 한다.\n","\n","**본 파일에서는 사전 훈련된 Bert 모델을 가지고 와서 우리의 task에 필요한 모델을 생성하고, 모델의 학습 과정에 필요한 초매개변수를 설정하는 과정을 다뤄볼 것이다.**"]},{"cell_type":"markdown","metadata":{"id":"9px4OyAKCDsa"},"source":["연습 문제를 시작하기에 앞서, 필요한 라이브러리들을 설치하자."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"9Tj07ECUMfN-"},"outputs":[],"source":["import torch\n","from transformers import get_linear_schedule_with_warmup, BertForSequenceClassification, BertConfig"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["'1.7.1+cu110'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["torch.__version__"]},{"cell_type":"markdown","metadata":{"id":"fUk91rnoCFrV"},"source":["감성 이진 분류 task를 위한 BertModel을 생성하자.\n","- 우선, 우리가 생성할 모델을 저장할 PATH를 지정하자.\n","- 이후, 중간 미션 task가 무엇인지 고려하여 HuggingFace 홈페이지에서 우리에게 필요한 BertModel 형식이 무엇인지 파악하자. 그리고 해당 모델을 이용하여 model을 생성하자. (*본 중간 미션이, 영화리뷰를 긍정과 부정의 두 가지 감정으로 분류해내는 작업임을 고려해보자.)\n","- 참고로 우리의 모델은 monologg의 kobert을 사전 훈련된 모델로 사용할 것이다. 사전 훈련 모델을 사용하기 위해서 from_pretrained 함수를 사용해야 하며, 함수의 파라미터에 'monologg/kobert'와 라벨 개수(num_labels)를 지정해야 한다.\n","- 모델을 생성하였다면, 미리 지정해둔 PATH에 해당 모델을 저장한다."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"GXFoQEm9-NUJ"},"outputs":[],"source":["# [분류를 위한 BERT 모델 생성: BertModel을 초기화하는 역할]\n","def BertModelInitialization():\n","    # PATH = \"/content/gdrive/MyDrive/BERT/model.pt\"\n","    PATH = \"model.pt\"\n","\n","    # BertModel은 다양한 작업을 진행할 수 있도록 여러 인터페이스들을 제공한다.\n","    # 본 중간 미션의 task가 '영화리뷰(Sequence)를 긍정과 부정의 두 가지 감정으로 분류하기(Classification)'이다.\n","    # Bert에서 시퀀스를 분류하는 인터페이스로 BertForSequenceClassification가 제공되고 있다.\n","    # 이 외 제공되는 인터페이스 종류가 궁금하다면 HuggingFace 홈페이지에 Bert를 검색해서 찾아보자: https://huggingface.co/docs/transformers/main/en/index\n","    # 추가로, monologg에 의해 사전훈련된 kobert 모델을 가지고 와야 하므로 .from_pretrained('monologg/kobert')를 쓰고, 분류 라벨 수가 2개 이므로 'num_labels=2'를 추가 입력한다.\n","\n","    model = BertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=2)\n","\n","\n","    torch.save(model.state_dict(), PATH) # 생성한 모델을 특정 PATH에 저장하기"]},{"cell_type":"markdown","metadata":{"id":"XJKsCyRZJgvk"},"source":["get_model은 생성한 BertModel을 불러와서, 그것을 우리의 디바이스에 등록하는 함수이다."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"q8oypFVGdt2X"},"outputs":[],"source":["def get_model(device):\n","    PATH = \"model.pt\"\n","\n","    model = BertForSequenceClassification.from_pretrained('monologg/kobert')\n","\n","    model.load_state_dict(torch.load(PATH)) # PATH에 저장된 모델을 불러오기\n","    model = model.to(device) # 불러온 모델을 device에 올리기\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"hFe5DnRsD9Gp"},"source":["이번에는, model의 학습 초매개변수(옵티마이저, 에포크, 훈련 스텝, 스케줄러)를 설정하는 **get_model_with_params** 함수를 정의하자.\n","- 우리는 학습 단계에서 최적의 매개변수를 찾아내기 위하여 옵티마이저를 사용할 수 있다. 대표적인 옵티마이저로 AdamW가 있고, 이 외에도 이하와 같이 다양한 옵티마이저들이 있다.\n","\n","> Batch Gradient Descent.\n","Stochastic Gradient Descent.\n","Momentum. Nesterov Accelerated Gradient\n","(NAG)\n","Adagrad.\n","RMSprop.\n","Adam.\n","\n","- 러닝 스케줄러는 학습이 이루어짐에 따라 learning_rate을 감소시키는 도구이다. 러닝 스케줄러 역시 get_linear_schedule_with_warmup 외에 다양한 종류가 있다.\n","\n","우선 중간미션에서는 AdamW으로 옵티마이저 종류 및 파라미터 내용, 학습 에폭 수, 총 훈련 스텝, 러닝 스케줄러를 모두 고정할 것이다. 최종 미션에서는 이러한 초매개변수를 직접 조절하면서 모델 성능 향상을 위한 다양한 방법을 시도해볼 것이다."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"BcFCBaNkXxrv"},"outputs":[],"source":["def get_model_with_params(num_data, device):\n","    model = get_model(device)\n","\n","    # 옵티마이저 설정하기\n","    optimizer = torch.optim.AdamW(model.parameters(),\n","                      lr = 1e-5, # 학습률\n","                      eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값,\n","                    )\n","    # 전체 데이터가 총 몇 번 학습되는지\n","    epochs = 3\n","\n","    # 총 훈련 스텝\n","    total_steps = num_data * epochs\n","\n","    # 학습이 이루어짐에 따라 learning_rate을 감소시키기 위한 스케줄러\n","    scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                                num_warmup_steps = 0,\n","                                                num_training_steps = total_steps)\n","\n","    return model, optimizer, scheduler, epochs\n"]},{"cell_type":"markdown","metadata":{"id":"P8QcibWcETxm"},"source":["**main** 함수를 통해, 다른 파일에서 import하기 전, model.ipynb 내에 정의된 함수가 성공적으로 구현되어 실행되는지 확인해보자."]},{"cell_type":"code","execution_count":19,"metadata":{"id":"ihB4OGzxscad"},"outputs":[],"source":["def main():\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    BertModelInitialization()\n","    print(get_model_with_params(200000, device))"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"KvhcU7f8Vdxy"},"outputs":[{"ename":"AttributeError","evalue":"module 'torch' has no attribute 'frombuffer'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m       main()\n","Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n\u001b[0;32m      2\u001b[0m     device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     BertModelInitialization()\n\u001b[0;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(get_model_with_params(\u001b[39m200000\u001b[39m, device))\n","Cell \u001b[1;32mIn[16], line 12\u001b[0m, in \u001b[0;36mBertModelInitialization\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m PATH \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodel.pt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[39m# BertModel은 다양한 작업을 진행할 수 있도록 여러 인터페이스들을 제공한다.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m# 본 중간 미션의 task가 '영화리뷰(Sequence)를 긍정과 부정의 두 가지 감정으로 분류하기(Classification)'이다.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m# Bert에서 시퀀스를 분류하는 인터페이스로 BertForSequenceClassification가 제공되고 있다.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m# 이 외 제공되는 인터페이스 종류가 궁금하다면 HuggingFace 홈페이지에 Bert를 검색해서 찾아보자: https://huggingface.co/docs/transformers/main/en/index\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m# 추가로, monologg에 의해 사전훈련된 kobert 모델을 가지고 와야 하므로 .from_pretrained('monologg/kobert')를 쓰고, 분류 라벨 수가 2개 이므로 'num_labels=2'를 추가 입력한다.\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m model \u001b[39m=\u001b[39m BertForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mmonologg/kobert\u001b[39;49m\u001b[39m'\u001b[39;49m, num_labels\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m     15\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), PATH)\n","File \u001b[1;32mc:\\Users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages\\transformers\\modeling_utils.py:2629\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2626\u001b[0m \u001b[39mif\u001b[39;00m from_pt:\n\u001b[0;32m   2627\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sharded \u001b[39mand\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2628\u001b[0m         \u001b[39m# Time to load the checkpoint\u001b[39;00m\n\u001b[1;32m-> 2629\u001b[0m         state_dict \u001b[39m=\u001b[39m load_state_dict(resolved_archive_file)\n\u001b[0;32m   2631\u001b[0m     \u001b[39m# set dtype to instantiate the model under:\u001b[39;00m\n\u001b[0;32m   2632\u001b[0m     \u001b[39m# 1. If torch_dtype is not None, we use that dtype\u001b[39;00m\n\u001b[0;32m   2633\u001b[0m     \u001b[39m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001b[39;00m\n\u001b[0;32m   2634\u001b[0m     \u001b[39m#    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\u001b[39;00m\n\u001b[0;32m   2635\u001b[0m     \u001b[39m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001b[39;00m\n\u001b[0;32m   2636\u001b[0m     dtype_orig \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages\\transformers\\modeling_utils.py:458\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[39melif\u001b[39;00m metadata[\u001b[39m\"\u001b[39m\u001b[39mformat\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    455\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    456\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion from a \u001b[39m\u001b[39m{\u001b[39;00mmetadata[\u001b[39m'\u001b[39m\u001b[39mformat\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m safetensors archive to PyTorch is not implemented yet.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    457\u001b[0m         )\n\u001b[1;32m--> 458\u001b[0m     \u001b[39mreturn\u001b[39;00m safe_load_file(checkpoint_file)\n\u001b[0;32m    459\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mload(checkpoint_file, map_location\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[1;32mc:\\Users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages\\safetensors\\torch.py:261\u001b[0m, in \u001b[0;36mload_file\u001b[1;34m(filename, device)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[39mwith\u001b[39;00m safe_open(filename, framework\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, device\u001b[39m=\u001b[39mdevice) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    260\u001b[0m     \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m f\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m--> 261\u001b[0m         result[k] \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mget_tensor(k)\n\u001b[0;32m    262\u001b[0m \u001b[39mreturn\u001b[39;00m result\n","\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'frombuffer'"]}],"source":["if __name__ == '__main__':\n","      main()"]},{"cell_type":"markdown","metadata":{"id":"tABh2EWAbbzt"},"source":["- https://pytorch.org/docs/stable/generated/torch.frombuffer.html#torch.frombuffer  \n","This function’s behavior is undefined when passed an object implementing the buffer protocol  \n","whose data is not on the CPU. Doing so is likely to cause a segmentation fault."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"cuda","language":"python","name":"cuda"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"}},"nbformat":4,"nbformat_minor":0}
