{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"4I5LBN5Zfl7Y"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting import_ipynb\n","  Downloading import_ipynb-0.1.4-py3-none-any.whl (4.1 kB)\n","Requirement already satisfied: IPython in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from import_ipynb) (8.12.0)\n","Requirement already satisfied: nbformat in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from import_ipynb) (5.9.0)\n","Requirement already satisfied: backcall in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from IPython->import_ipynb) (0.2.0)\n","Requirement already satisfied: decorator in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from IPython->import_ipynb) (5.1.1)\n","Requirement already satisfied: jedi>=0.16 in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from IPython->import_ipynb) (0.18.1)\n","Requirement already satisfied: matplotlib-inline in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from IPython->import_ipynb) (0.1.6)\n","Requirement already satisfied: pickleshare in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from IPython->import_ipynb) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from IPython->import_ipynb) (3.0.36)\n","Requirement already satisfied: pygments>=2.4.0 in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from IPython->import_ipynb) (2.15.1)\n","Requirement already satisfied: stack-data in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from IPython->import_ipynb) (0.2.0)\n","Requirement already satisfied: traitlets>=5 in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from IPython->import_ipynb) (5.7.1)\n","Requirement already satisfied: typing-extensions in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from IPython->import_ipynb) (3.7.4.3)\n","Requirement already satisfied: colorama in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from IPython->import_ipynb) (0.4.6)\n","Requirement already satisfied: fastjsonschema in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from nbformat->import_ipynb) (2.17.1)\n","Requirement already satisfied: jsonschema>=2.6 in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from nbformat->import_ipynb) (4.17.3)\n","Requirement already satisfied: jupyter-core in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from nbformat->import_ipynb) (5.3.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from jedi>=0.16->IPython->import_ipynb) (0.8.3)\n","Requirement already satisfied: attrs>=17.4.0 in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from jsonschema>=2.6->nbformat->import_ipynb) (23.1.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.19.3)\n","Requirement already satisfied: wcwidth in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->IPython->import_ipynb) (0.2.5)\n","Requirement already satisfied: platformdirs>=2.5 in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from jupyter-core->nbformat->import_ipynb) (2.5.2)\n","Requirement already satisfied: pywin32>=300 in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from jupyter-core->nbformat->import_ipynb) (305.1)\n","Requirement already satisfied: executing in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from stack-data->IPython->import_ipynb) (0.8.3)\n","Requirement already satisfied: asttokens in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from stack-data->IPython->import_ipynb) (2.0.5)\n","Requirement already satisfied: pure-eval in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from stack-data->IPython->import_ipynb) (0.2.2)\n","Requirement already satisfied: six in c:\\users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages (from asttokens->stack-data->IPython->import_ipynb) (1.15.0)\n","Installing collected packages: import_ipynb\n","Successfully installed import_ipynb-0.1.4\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: Error parsing requirements for matplotlib: [Errno 2] No such file or directory: 'c:\\\\users\\\\com\\\\anaconda3\\\\envs\\\\cuda\\\\lib\\\\site-packages\\\\matplotlib-3.7.1.dist-info\\\\METADATA'\n"]}],"source":["!pip install import_ipynb"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"nTqcsIWyfqLQ"},"outputs":[{"name":"stdout","output_type":"stream","text":["importing Jupyter notebook from Preproc.ipynb\n"]},{"name":"stderr","output_type":"stream","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n","The class this function is called from is 'KoBertTokenizer'.\n"]},{"name":"stdout","output_type":"stream","text":["[   2  517 7385 6312 5439 7766 7096 6022  770 6312 5439 7766 7096 6022\n"," 1115 6416 7794 5384 3692 7728 5561 7864 6855    3    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0]\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","importing Jupyter notebook from model.ipynb\n"]}],"source":["import import_ipynb, torch\n","\n","from Preproc import data_to_token_ids, token_ids_to_mask\n","from model import get_model\n","from tokenization import KoBertTokenizer"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"7AQdbcbOsHQk"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8822fcb214144e58b8038197096e3767","version_major":2,"version_minor":0},"text/plain":["Downloading model.safetensors:   0%|          | 0.00/369M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\com\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n","To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n","  warnings.warn(message)\n"]},{"ename":"AttributeError","evalue":"module 'torch' has no attribute 'frombuffer'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# GPU 디바이스 설정\u001b[39;00m\n\u001b[0;32m      2\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m get_model(device)\n","File \u001b[1;32m<string>:4\u001b[0m, in \u001b[0;36mget_model\u001b[1;34m(device)\u001b[0m\n","File \u001b[1;32mc:\\Users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages\\transformers\\modeling_utils.py:2629\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2626\u001b[0m \u001b[39mif\u001b[39;00m from_pt:\n\u001b[0;32m   2627\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sharded \u001b[39mand\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2628\u001b[0m         \u001b[39m# Time to load the checkpoint\u001b[39;00m\n\u001b[1;32m-> 2629\u001b[0m         state_dict \u001b[39m=\u001b[39m load_state_dict(resolved_archive_file)\n\u001b[0;32m   2631\u001b[0m     \u001b[39m# set dtype to instantiate the model under:\u001b[39;00m\n\u001b[0;32m   2632\u001b[0m     \u001b[39m# 1. If torch_dtype is not None, we use that dtype\u001b[39;00m\n\u001b[0;32m   2633\u001b[0m     \u001b[39m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001b[39;00m\n\u001b[0;32m   2634\u001b[0m     \u001b[39m#    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\u001b[39;00m\n\u001b[0;32m   2635\u001b[0m     \u001b[39m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001b[39;00m\n\u001b[0;32m   2636\u001b[0m     dtype_orig \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages\\transformers\\modeling_utils.py:458\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[39melif\u001b[39;00m metadata[\u001b[39m\"\u001b[39m\u001b[39mformat\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    455\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    456\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion from a \u001b[39m\u001b[39m{\u001b[39;00mmetadata[\u001b[39m'\u001b[39m\u001b[39mformat\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m safetensors archive to PyTorch is not implemented yet.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    457\u001b[0m         )\n\u001b[1;32m--> 458\u001b[0m     \u001b[39mreturn\u001b[39;00m safe_load_file(checkpoint_file)\n\u001b[0;32m    459\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mload(checkpoint_file, map_location\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[1;32mc:\\Users\\com\\anaconda3\\envs\\cuda\\lib\\site-packages\\safetensors\\torch.py:261\u001b[0m, in \u001b[0;36mload_file\u001b[1;34m(filename, device)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[39mwith\u001b[39;00m safe_open(filename, framework\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, device\u001b[39m=\u001b[39mdevice) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    260\u001b[0m     \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m f\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m--> 261\u001b[0m         result[k] \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mget_tensor(k)\n\u001b[0;32m    262\u001b[0m \u001b[39mreturn\u001b[39;00m result\n","\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'frombuffer'"]}],"source":["# GPU 디바이스 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = get_model(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kwiuyp0BruXy"},"outputs":[],"source":["while True:\n","  input_str = input()\n","\n","  # q를 입력하면 while 루프 끝\n","  if input_str == \"q\":\n","    break\n","\n","  # 입력받은 문장을 토큰화\n","  tokenizer = KoBertTokenizer.from_pretrained(\"monologg/kobert\")\n","  tokenized_data = [data_to_token_ids(tokenizer, input_str)]\n","  attention_masks = [token_ids_to_mask(token_ids) for token_ids in tokenized_data]\n","\n","  # 토큰을 텐서로 변환\n","  inputs_tensor = torch.tensor(tokenized_data).to(device)\n","  masks_tensor = torch.tensor(attention_masks).to(device)\n","\n","  # 신경망의 Forward 함수 활용\n","  outputs = ------------------------------------\n","\n","  # 확률 값에 따른 문장 분류\n","  logits = outputs[0][0]\n","\n","  # 조건문을 활용하여 감정 이진 분류 결과를 출력하라.\n","  # 각 라벨이 어떤 말을 의미하는지 생각해보자.\n","  # 만약 \"나는 당신을 정말로 사랑합니다\" 라는 말을 입력하면 \"당신이 입력한 문장은 긍정입니다.\"라는 문구가 출력되어야 하고\n","  # \"화가 너무 난다. 진짜 짜증나네\" 라는 말을 입력하면 \"당신이 입력한 문장은 부정입니다.\"라는 문구가 출력되어야 합니다.\n","  ## 여기에 코드 작성\n","  ------------------------------------\n","  ------------------------------------\n","  ------------------------------------\n","  ------------------------------------\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bLLCW1asvXXC"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"cuda","language":"python","name":"cuda"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"}},"nbformat":4,"nbformat_minor":0}
