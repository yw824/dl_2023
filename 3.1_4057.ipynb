{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5116be8",
   "metadata": {
    "id": "c5116be8"
   },
   "source": [
    "# 실습 5: Universal Function Approximator 학습시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8237930f",
   "metadata": {
    "id": "8237930f"
   },
   "source": [
    "Universal Approximation Theorem에 의거하여 보편적인 함수 $f$의 근사함수 $F$의 모델을 $ F(\\{v_i\\}, \\{\\textbf{w}_i\\}, \\{b_i\\}; \\textbf{x}) = \\sum_{i=1}^{N} v_{i}\\varphi(\\textbf{x} \\cdot \\textbf{w}_{i} + b_{i})$ 로 세우고 학습을 진행해볼 것이다.\n",
    "\n",
    "학습용 데이터셋을 $\\textbf{x}$와 $\\textbf{x}$에 대응되는 truth 데이터 $f(\\textbf{x})$의 데이터 쌍들로 구성한 후, 경사하강법을 이용하여 적절한 모델 파라미터 $\\{v_i\\}, \\{w_i\\}, \\{b_i\\}$ 를 구함으로써 보편적인 수학적 함수 $f$의 근사함수를 학습시킬 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d947db",
   "metadata": {
    "id": "13d947db"
   },
   "source": [
    "### 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ced2e4",
   "metadata": {
    "id": "f0ced2e4"
   },
   "source": [
    "이번 실습에서는 일변수 함수인 $f(x)=cos(x), x \\in [-2\\pi, 2\\pi]$의 근사함수를 구해볼 것이다. 여기서 $f$가 일변수 스칼라 함수이기 때문에 $x$, $w_i$를 벡터가 아닌 스칼라로 생각할 수 있다. 그러나 추후에 고차원 벡터(길이가 긴 1차원 배열)를 입력받는 모델에 대해서도 확장하기 위해, 프로그래밍할 때 스칼라가 아닌 벡터(즉, 길이가 1인 1차원 배열)라고 생각하고 프로그래밍 해보자.\n",
    "\n",
    "주의) 여기서 세 종류의 모델 파라미터 각각의 개수이자 뉴런의 개수인 $N$은 하이퍼파라미터에 속한다. 즉, 학습을 통해 컴퓨터가 직접 적절한 값을 찾아가는 것이 아니라, 인간이 직접 시행착오를 통해 적절히 찾아 지정해주어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3f20e5",
   "metadata": {
    "id": "bc3f20e5"
   },
   "source": [
    "그동안의 실습에서 꾸준히 사용해온 대표적인 라이브러리들을 미리 import 한 후 본격적인 실습을 시작해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5a0c10",
   "metadata": {
    "id": "4b5a0c10",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mygrad as mg\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29407ff5",
   "metadata": {
    "id": "29407ff5"
   },
   "source": [
    "### Step1. 모델(클래스) 정의하기 : 순전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c5f817",
   "metadata": {
    "id": "f2c5f817"
   },
   "source": [
    "우리가 가장 먼저 할 일은 Universal Approximation Theorem에 따라 구하게 될 근사함수 $F(x)$의 형태를 모델(클래스)로 정의해주는 것이다. 선형모델에 비해 구현하기 까다롭지만, MyGrad와 NumPy 라이브러리의 벡터화된 연산을 이용하면 짧고 간단한 코드를 작성할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d80f130",
   "metadata": {
    "id": "4d80f130"
   },
   "source": [
    "#### batch의 개념 짚고 넘어가기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ce32c7",
   "metadata": {
    "id": "86ce32c7"
   },
   "source": [
    "우리는 입력 데이터를 $M$개씩 묶어 batch $\\{x_{j}\\}_{j=1}^{M}$로 구성한 후, batch를 이루는 개별적인 데이터에 대해 독립적으로 순전파를 진행할 것이다. 즉, $M$개의 데이터를 모델에 한 번에 전달하여 대응되는 $M$개의 예측을 한 번에 산출할 것이다.\n",
    "\n",
    "batch 내 개별 데이터에 대한 함숫값 $ F(\\{v_i\\}, \\{{w}_i\\}, \\{b_i\\}; x_j)$를 구할 때 사용되는 연산은 batch 내의 다른 데이터들에는 영향을 받지 않는 독립적인 연산임을 꼭 명심하자. 각 예측은 대응되는 입력 데이터에 대해서만 이루어지기 때문에 다음의 값들에만 의존한다.\n",
    "\n",
    "- 모델 파라미터: $\\{v_i\\}, \\{w_i\\}, \\{b_i\\}$ \n",
    "- 데이터 $x_j$\n",
    "\n",
    "\n",
    "하나의 batch 당 $M$개의 독립적인 예측을 진행해야 한다고 하면 흔히 for문 등의 반복문을 떠올릴 것이다. 그러나 batch를 이루는 입력 데이터 $M$개를 shape-$(M, 1)$인 NumPy 배열로 정의하면 NumPy의 벡터화된 연산을 사용하여 순전파를 한 번에 진행할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532fa954",
   "metadata": {
    "id": "532fa954"
   },
   "source": [
    "#### 근사함수 수식을 코딩으로 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94165d0c",
   "metadata": {
    "id": "94165d0c"
   },
   "source": [
    "이제 근사함수 $ F(\\{v_i\\}, \\{{w}_i\\}, \\{b_i\\}; x) = \\sum_{i=1}^{N} v_{i}\\varphi(x \\cdot {w}_{i} + b_{i})$ 의 식을 batch size가 $M$인 batch에 대해 어떻게 코딩할지 고민해보자.\n",
    "\n",
    "먼저, $x \\cdot w_{i}$는 MyGrad 라이브러리의 matmul을 이용하여 계산할 수 있다. 이 문제에서 $x$와 $w_i$는 1차원 벡터이다. 따라서 $x \\cdot w_i$는 벡터 간의 내적을 뜻하지만, 동시에 행벡터와 열벡터의 행렬곱으로도 생각할 수 있기 때문에 matmul 연산을 사용할 수 있다.\n",
    "\n",
    "행렬곱을 이용하여 $N$개의 $w_i$에 대해 $x \\cdot w_{i}$를 한번에 계산해보자. 1차원 행벡터 $x$를 1차원 열벡터 $w_i$ $N$개를 열에 대해 늘어놓은 행렬 $W$와 행렬곱하면 다음과 같다.\n",
    "\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\begin{pmatrix}x\\end{pmatrix}\n",
    "\\begin{pmatrix}w_{1} & w_{2} & \\cdots & w_N \\end{pmatrix} = \n",
    "\\begin{pmatrix}x \\cdot w_{1} & x \\cdot w_{2} & \\cdots & x \\cdot w_N \\end{pmatrix}\n",
    "\\end{equation}$$\n",
    "\n",
    "결과적으로, $N$개의 $x \\cdot w_{i}$ 값이 담긴 shape-$(1, N)$인 행렬을 얻게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6abbc5",
   "metadata": {
    "id": "2d6abbc5"
   },
   "source": [
    "이제 batch를 이루는 $M$개의 입력 데이터 $x_j$에 대해 위의 행렬곱을 한 번에 계산해보자. 1차원 행벡터 $x_j$를 행에 대해 늘어놓은 행렬 $X$를 1차원 열벡터 $w_i$ N개를 열에 대해 늘어놓은 행렬 $W$와 행렬곱하면 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}x_1\\\\\n",
    "x_2\\\\\n",
    "\\vdots\\\\\n",
    "x_M\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}w_{1} & w_{2} & \\cdots & w_N \\end{pmatrix} = \n",
    "\\begin{pmatrix}x_1 \\cdot w_{1} & x_1 \\cdot w_{2} & \\cdots & x_1 \\cdot w_N \\\\ \n",
    "x_2 \\cdot w_{1} & x_2 \\cdot w_{2} & \\cdots & x_2 \\cdot w_N \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_M \\cdot w_{1} & x_M \\cdot w_{2} & \\cdots & x_M \\cdot w_N \n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "결과적으로, $MN$개의 $x_{j} \\cdot w_{i}$ 값이 담긴 shape-$(M, N)$인 행렬을 얻게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792800d6",
   "metadata": {
    "id": "792800d6"
   },
   "source": [
    "결국, $X$에 shape-$(M,1)$, $W$에 shape-$(1,N)$의 적절한 모양의 배열만 넣어주면 한 번의 행렬곱(mg.matmul) 계산으로 원하는 결과를 얻을 수 있음을 알 수 있다. 따라서 $F(\\{v_i\\}, \\{w_i\\}, \\{b_i\\}; x_j ) = \\sum_{i=1}^{N} v_{i}\\varphi(x_{j} \\cdot w_{i} + b_{i})$ 에서 $x_{j} \\cdot w_{i}$ 부분을 구하는 코드는 다음과 같다.\n",
    "\n",
    "```python\n",
    "mg.matmul(x, w)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3973ba",
   "metadata": {
    "id": "ea3973ba"
   },
   "source": [
    "그 다음으로는 브로드캐스팅 성질을 이용한다. 우리는 스칼라 $b_i$가 shape-$(M, N)$인 행렬의 i번째 열 전체에 각각 더해지길 바란다.\n",
    "\n",
    "\\begin{pmatrix}x_1 \\cdot w_{1}+b_1 & x_1 \\cdot w_{2}+b_2 & \\cdots & x_1 \\cdot w_N+b_N \\\\ \n",
    "x_2 \\cdot w_{1}+b_1 & x_2 \\cdot w_{2}+b_2 & \\cdots & x_2 \\cdot w_N+b_N \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_M \\cdot w_{1}+b_1 & x_M \\cdot w_{2}+b_2 & \\cdots & x_M \\cdot w_N+b_N \n",
    "\\end{pmatrix}\n",
    "\n",
    "$\\{b_i\\}$를 shape-$(1,N)$인 행렬로 정의해주면, NumPy와 MyGrad의 브로드캐스팅 성질에 의해 다음의 코드 한 줄로 해결이 된다. 즉, $x_{j} \\cdot w_{i} + b_{i}$ 부분을 구하는 코드는 다음과 같다.\n",
    "\n",
    "```python\n",
    "mg.matmul(x, w)+b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e943d",
   "metadata": {
    "id": "524e943d"
   },
   "source": [
    "그 다음, 우리는 행렬의 각 원소에 대한 시그모이드 함숫값을 구해야 한다.\n",
    "\n",
    "\\begin{pmatrix}\\varphi(x_1 \\cdot w_{1}+b_1) & \\varphi(x_1 \\cdot w_{2}+b_2) & \\cdots & \\varphi(x_1 \\cdot w_N+b_N) \\\\ \n",
    "\\varphi(x_2 \\cdot w_{1}+b_1) & \\varphi(x_2 \\cdot w_{2}+b_2) & \\cdots & \\varphi(x_2 \\cdot w_N+b_N) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\varphi(x_M \\cdot w_{1}+b_1) & \\varphi(x_M \\cdot w_{2}+b_2) & \\cdots & \\varphi(x_M \\cdot w_N+b_N) \n",
    "\\end{pmatrix}\n",
    "\n",
    "NumPy와 MyGrad에서 수학 연산을 하는 함수에 NumPy 배열 혹은 MyGrad 텐서를 넣어주면 행렬의 모든 원소에 수학 연산을 각각 적용한 행렬을 얻을 수 있다. sigmoid 함수는 앞서 살펴본대로 mygrad.nnet.activations에서 import 할 수 있다. 따라서 $\\varphi(x_{j} \\cdot w_{i} + b_{i})$ 부분을 구하는 코드는 다음과 같다.\n",
    "\n",
    "```python\n",
    "sigmoid(mg.matmul(x, w)+b)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d600972b",
   "metadata": {
    "id": "d600972b"
   },
   "source": [
    "마지막으로 우리는 위에서 구한 행렬의 각 행 별로 $i$번째 원소에 $v_i$를 곱한 후 $(i=1,2, \\cdots, N)$에 대해 다 더해주어야 한다. 그래서 최종적으로 다음과 같은 shape-$(M,1)$인 행렬을 구하기를 바란다.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}\\sum_{i=1}^{N} v_{i}\\varphi(x_{1} \\cdot w_{i} + b_{i}) \\\\ \n",
    "\\sum_{i=1}^{N} v_{i}\\varphi(x_{2} \\cdot w_{i} + b_{i}) \\\\\n",
    "\\vdots \\\\\n",
    "\\sum_{i=1}^{N} v_{i}\\varphi(x_{M} \\cdot w_{i} + b_{i})\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "이 행렬을 행렬곱 형태로 나타내면 다음과 같다. $\\{v_i\\}$는 shape-$(N,1)$인 배열로 정의해주어야 함을 확인할 수 있다.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}\\varphi(x_1 \\cdot w_{1}+b_1) & \\varphi(x_1 \\cdot w_{2}+b_2) & \\cdots & \\varphi(x_1 \\cdot w_N+b_N) \\\\ \n",
    "\\varphi(x_2 \\cdot w_{1}+b_1) & \\varphi(x_2 \\cdot w_{2}+b_2) & \\cdots & \\varphi(x_2 \\cdot w_N+b_N) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\varphi(x_M \\cdot w_{1}+b_1) & \\varphi(x_M \\cdot w_{2}+b_2) & \\cdots & \\varphi(x_M \\cdot w_N+b_N) \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}v_1\\\\\n",
    "v_2\\\\\n",
    "\\vdots\\\\\n",
    "v_N\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\\sum_{i=1}^{N} v_{i}\\varphi(x_{1} \\cdot w_{i} + b_{i}) \\\\ \n",
    "\\sum_{i=1}^{N} v_{i}\\varphi(x_{2} \\cdot w_{i} + b_{i}) \\\\\n",
    "\\vdots \\\\\n",
    "\\sum_{i=1}^{N} v_{i}\\varphi(x_{M} \\cdot w_{i} + b_{i})\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "이로써 batch size가 $M$인 입력데이터 $\\{x_{j}\\}_{j=1}^{M}$에 대해 $F(\\{v_i\\}, \\{w_i\\}, \\{b_i\\}; x_j ) = \\sum_{i=1}^{N} v_{i}\\varphi(x_{j} \\cdot w_{i} + b_{i})$을 한 번에 계산하는 벡터화 연산 코드를 찾아낼 수 있었다.\n",
    "\n",
    "```python\n",
    "out1 = sigmoid(mg.matmul(x, w) + b)  # shape-(M,N)\n",
    "model_out = mg.matmul(out1, v)       # shape-(M,1)\n",
    "```\n",
    "\n",
    "model_out이 shape-$(M, 1)$인 텐서이므로, batch size가 $M$인 경우 대응되는 예측값이 총 $M$개의 독립적인 값으로 나타난다는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7f80a3",
   "metadata": {
    "id": "6b7f80a3"
   },
   "source": [
    "#### 모델(클래스) 정의하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fc400a",
   "metadata": {
    "id": "37fc400a"
   },
   "source": [
    "이제, 아래의 Model 클래스를 완성해보자. 모델 파라미터 $\\{w_i\\}$, $\\{b_i\\}$, $\\{v_i\\}$가 어떤 형태이고 왜 그런 형태여야 하는지 이해한 것을 바탕으로 initialize_params() 메서드를 작성할 수 있을 것이다. 그리고 model_out의 식을 어떻게 프로그래밍하는지 이해했다면, \\_\\_call\\_\\_() 메서드를 작성할 수 있을 것이다. 주석을 보며 스스로 작성해보자.\n",
    "(load_parameters 메서드는 학습 과정에 필요한 메서드는 아니고, Step 5에서 학습 과정을 시각적으로 확인하는 데 사용하기 위해 정의해둔 메서드이다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b896d6",
   "metadata": {
    "id": "70b896d6",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from mygrad.nnet.initializers import normal\n",
    "from mygrad.nnet.activations import sigmoid\n",
    "\n",
    "class Model:\n",
    "\n",
    "    def initialize_params(self, num_neurons: int):\n",
    "        self.w = normal(1, num_neurons)\n",
    "        self.b = normal(num_neurons)\n",
    "        self.v = normal(num_neurons, 1)\n",
    "\n",
    "\n",
    "    def __init__(self, num_neurons: int):\n",
    "        # self.N 초기화 : `num_neurons`\n",
    "        self.N = num_neurons\n",
    "\n",
    "        # `self.w`, `self.b`, self.v` 초기화 : `self.initialize_params()` 이용 \n",
    "        self.initialize_params(num_neurons)\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        out1 = sigmoid(x @ self.w + self.b)  # matmul[(M,1) w/ (1, N)] + (N,) --> (M, N)\n",
    "        return out1 @ self.v # matmul[(M, N) w/ (N, 1)] --> (M, 1)\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return (self.w, self.b, self.v)  \n",
    "\n",
    "\n",
    "    def load_parameters(self, w, b, v):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.v = v\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219aab9a",
   "metadata": {
    "id": "219aab9a"
   },
   "source": [
    "### Step2. 손실함수 작성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f52fbaa",
   "metadata": {
    "id": "6f52fbaa"
   },
   "source": [
    "모델을 정의했으므로 우리는 순전파를 통해 초기 근사함수 $F(x)$의 값을 얻을 수 있다. 초기 근사함수 $F(x)$는 임의로 지정한 모델 파라미터에 의해 정의된 함수이므로, 당연히 $f(x)=cos(x)$와는 차이가 클 것이다. 그러나 우리는 Universal Approximation Theorem에 의해, 근사함수 $F(\\textbf x)$가 임의의 $\\varepsilon > 0$에 대해 $ | F( \\textbf{x} ) - f ( \\textbf{x} ) | < \\varepsilon $를 만족하도록 만드는 모델 파라미터 값들이 반드시 존재한다는 것을 알고 있다. 따라서, 손실함수를 $\\mathscr{L}(x) = | F(x) - f(x) | = | F( \\{v_i\\}, \\{w_i\\}, \\{b_i\\}; x ) - \\cos ( x ) |$ 로 정의하여, 손실함수 값이 0에 가까워지도록 학습하는 것이 자연스럽다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193614d7",
   "metadata": {
    "id": "193614d7"
   },
   "source": [
    "그런데 우리는 batch size가 $M$인 batch를 모델에 한 번에 입력해줄 것이므로, $M$개의 손실함수 값 $\\mathscr{L}(x_j)= | F( \\{v_i\\}, \\{w_i\\}, \\{b_i\\}; x_j ) - \\cos ( x_j ) | \\quad(j = 1, \\dots, M)$을 한 번에 얻게된다. 우리는 이렇게 얻은 $M$개의 손실에 대해 평균을 구하여, 평균적인 손실을 학습에 사용할 것이다. batch 전체의 평균적인 손실을 학습에 사용하면 모델의 가중치를 더 연속적이고 부드럽게 이동시킬 수 있다.\n",
    "\n",
    "$L(\\{v_i\\}, \\{w_i\\}, \\{b_i\\}; \\{x_k\\} ) = \\frac{1}{M}\\sum_{j=1}^{M} | F(\\{v_i\\}, \\{w_i\\}, \\{b_i\\}; x_j ) - \\cos ( x_{j} ) |$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b6385b",
   "metadata": {
    "id": "e9b6385b"
   },
   "source": [
    "다음 코드의 주석을 잘 보고, 손실함수를 작성해보자. MyGrad의 수학 연산 함수들을 이용하여 단 한 줄의 코드로 완성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cd31e7",
   "metadata": {
    "id": "37cd31e7",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def loss_func(pred, true):\n",
    "    return mg.mean(mg.abs(pred - true)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8d7f34",
   "metadata": {
    "id": "ee8d7f34"
   },
   "source": [
    "### Step3. 경사하강법 함수 작성하기 : 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a233c459",
   "metadata": {
    "id": "a233c459"
   },
   "source": [
    "모델 파라미터에 해당하는 텐서들과 학습률을 매개변수로 받아 경사하강법을 진행하는 함수를 작성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d340eb6e",
   "metadata": {
    "id": "d340eb6e",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_step(tensors, learning_rate):\n",
    "    if isinstance(tensors, mg.Tensor):\n",
    "        # Only one tensor was provided. Pack\n",
    "        # it into a list so it can be accessed via\n",
    "        # iteration\n",
    "        tensors = [tensors]\n",
    "\n",
    "    for t in tensors:\n",
    "        if t.grad is not None:\n",
    "            t.data -= learning_rate * t.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a668ea8",
   "metadata": {
    "id": "3a668ea8"
   },
   "source": [
    "### Step4. 학습시키기 : 순전파와 역전파의 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6737415b",
   "metadata": {
    "id": "6737415b"
   },
   "source": [
    "#### 데이터 준비하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a5c6de",
   "metadata": {
    "id": "e5a5c6de"
   },
   "source": [
    "함수 $f(x)$의 근사함수를 구하는 상황이므로 데이터셋을 준비하는 것이 무척 간단하다. 학습용 데이터셋은 $(x_n, y_n^{(true)})$ 쌍으로 구성하면 되는데, 이 문제 상황에서는 $y_n^{(true)} = cos(x_n)$이다. 예를 들어 구간 $[-2\\pi, 2\\pi]$에서 근사함수를 구하고 싶다면, 구간 $[-2\\pi, 2\\pi]$를 균등하게 1000개 정도의 구간으로 나눈 배열을 입력되는 $x$데이터(train_data)로 사용하고, $cos$ 연산을 적용하는 함수를 true_f로 정의한 후, truth_data 배열을 true_f에 통과하여 얻어진 배열을 truth 데이터로 사용하면 될 것이다.\n",
    "\n",
    "이때, 학습용 데이터를 이루는 $x_n$ 값들은 NumPy 배열이나 MyGrad의 상수 텐서로 정의하여 .backward() 메서드의 수행과정에서 불필요한 편미분계수가 계산되지 않도록 해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a56767",
   "metadata": {
    "id": "b8a56767"
   },
   "source": [
    "#### 학습시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b311e9",
   "metadata": {
    "id": "18b311e9"
   },
   "source": [
    "학습시키기에 앞서 train_data와 truth를 잘 정의해주어야 한다. 또한, Step 1에서 정의한 Model 클래스의 객체 model을 생성하여 모델 파라미터들이 초기화되도록 해야 한다.\n",
    "\n",
    "주석을 잘 보고 데이터와 모델을 준비해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3e7884",
   "metadata": {
    "id": "8d3e7884",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# shape-(1000,1)인 학습용 데이터셋 {x_n} 정의하기\n",
    "# NumPy 배열이나 MyGrad의 상수 텐서로 정의해야 함\n",
    "# 배열의 이름은 train_data로 지정하기\n",
    "\n",
    "train_data = np.linspace(-2*np.pi, 2*np.pi, 1000).reshape(1000, 1)\n",
    "\n",
    "\n",
    "# true_f(x) 정의하기\n",
    "# x를 입력받아 np.cos(x)를 반환하는 함수로 정의하면 됨\n",
    "# y의 truth를 true_f(train_data) 와 같이 구할 수 있도록\n",
    "\n",
    "def true_f(x): return np.cos(x)  \n",
    "\n",
    "\n",
    "# 위에서 작성한 Model 클래스 이용하여 model 객체 생성하기\n",
    "# 하이퍼파라미터인 뉴런 개수 num_neurons=10 으로 시작\n",
    "\n",
    "model = Model(num_neurons=10) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1179d6f7",
   "metadata": {
    "id": "1179d6f7"
   },
   "source": [
    "학습시키기 전, 초기화된 상태의 model은 우리가 근사하고자 하는 목표 함수인 $f(x) = cos(x)$와 크게 다르다. 함수 $f$와 학습시키기 전의 근사함수 모델 $F$가 어떻게 다른지 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcad4e85",
   "metadata": {
    "id": "dcad4e85",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# \"True function: f\" 와 근사함수인 \"Approximating function: F\"를 같은 축 상에 그래프로 나타내기\n",
    "# 위에서 정의한 true_f 함수와 초기화된 model 객체를 잘 활용하기\n",
    "# 두 그래프가 각각 어떤 그래프인지 알아보기 위해 label 설정은 필수, 색상 설정은 자유롭게\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_data, true_f(train_data), label=\"True function: f\") \n",
    "ax.plot(train_data, model(train_data).data, label=\"Approximating function: F\") \n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b72fa6",
   "metadata": {
    "id": "69b72fa6"
   },
   "source": [
    "이제 본격적으로 학습을 진행할 것이다. 하이퍼파라미터인 batch size, 학습률, epoch 수 등을 지정해준 상태에서 학습을 시작한다. 매 epoch마다 랜덤 batch 별 학습을 진행하기 위해서 batch를 먼저 정해주고, 각 batch에 대해 한 번에 학습을 진행하는 방식으로 프로그래밍하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc7542",
   "metadata": {
    "id": "72fc7542",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "learning_rate = 0.01 \n",
    "\n",
    "for epoch_cnt in range(1000):\n",
    "    idxs = np.arange(len(train_data))\n",
    "    np.random.shuffle(idxs)\n",
    "\n",
    "    for batch_cnt in range(0, len(train_data) // batch_size):\n",
    "        batch_indices = idxs[batch_cnt * batch_size : (batch_cnt + 1) * batch_size] \n",
    "        batch = train_data[batch_indices]\n",
    "\n",
    "        truth = true_f(batch)  \n",
    "\n",
    "        prediction = model(batch)  \n",
    "\n",
    "        loss = loss_func(prediction, truth) \n",
    "\n",
    "        loss.backward() \n",
    "\n",
    "        gradient_step(model.parameters, learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef44de0",
   "metadata": {
    "id": "2ef44de0"
   },
   "source": [
    "학습시킨 후의 model은 $f(x) = cos(x)$와 꽤 비슷해진다. 아래 코드를 완성하여 학습시킨 후의 근사함수 모델 $F$가 함수 $f$와 얼마나 가까워졌는지 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ecf6e3",
   "metadata": {
    "id": "80ecf6e3",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = train_data\n",
    "ax.plot(train_data, true_f(train_data), label=\"True function: f\")\n",
    "ax.plot(train_data, model(train_data).data, label=\"Approximating function: F\")\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4762a5e6",
   "metadata": {
    "id": "4762a5e6"
   },
   "source": [
    "### Step5. 학습 과정 다양하게 관찰하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3329069f",
   "metadata": {
    "id": "3329069f"
   },
   "source": [
    "#### 학습 진행에 따른 손실의 변화를 실시간으로 관찰하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55858a2a",
   "metadata": {
    "id": "55858a2a"
   },
   "source": [
    "학습이 진행됨에 따라 손실이 어떻게 변화하는지 확인하기 위해 noggin 라이브러리의 create_plot 함수를 사용해볼 것이다. 다음의 코드와 같이 create_plot() 함수는 plotter, fig, ax를 반환한다. 여기서 metrics=[\"loss\"] 로 설정해주었기 때문에, plotter는 학습 과정을 진행하는 동안 2초마다 손실의 변화를 추적하는 역할을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a263a1",
   "metadata": {
    "id": "c0a263a1",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from noggin import create_plot\n",
    "\n",
    "plotter, fig, ax = create_plot(metrics=[\"loss\"])\n",
    "ax.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6aa792",
   "metadata": {
    "id": "7c6aa792"
   },
   "source": [
    "이제 위에서 진행했던 학습을 다시 진행할 것이다. 그런데 이번에는 Step 4의 학습 코드의 적절한 위치에 아래의 두 코드를 넣어 다시 실행해보자. epoch를 반복하는 과정, batch 별 학습을 반복하는 과정이 진행되는 각 for문에 넣으면 된다. 그러면 학습 중에 loss를 추적할 수 있게 된다. \n",
    "\n",
    "```python\n",
    "    plotter.set_train_batch({\"loss\": loss.item()}, batch_size=batch_size)\n",
    "```\n",
    "\n",
    "```python\n",
    "    plotter.set_train_epoch()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9caae5",
   "metadata": {
    "id": "cb9caae5"
   },
   "source": [
    "마지막으로 학습이 끝난 위치에 다음 코드를 추가해줌으로써, 누락된 데이터가 빠짐없이 기록되도록 할 수 있다.\n",
    "\n",
    "```python\n",
    "    plotter.plot()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc2628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "learning_rate = 0.01 \n",
    "\n",
    "for epoch_cnt in range(1000):\n",
    "    idxs = np.arange(len(train_data))\n",
    "    np.random.shuffle(idxs)\n",
    "    plotter.set_train_batch({\"loss\": loss.item()}, batch_size=batch_size)\n",
    "    plotter.set_train_epoch()\n",
    "\n",
    "    for batch_cnt in range(0, len(train_data) // batch_size):\n",
    "        batch_indices = idxs[batch_cnt * batch_size : (batch_cnt + 1) * batch_size] \n",
    "        batch = train_data[batch_indices]\n",
    "\n",
    "        truth = true_f(batch)  \n",
    "\n",
    "        prediction = model(batch)  \n",
    "\n",
    "        loss = loss_func(prediction, truth) \n",
    "\n",
    "        loss.backward() \n",
    "\n",
    "        gradient_step(model.parameters, learning_rate) \n",
    "        \n",
    "        plotter.set_train_batch({\"loss\": loss.item()}, batch_size=batch_size)\n",
    "        plotter.set_train_epoch()\n",
    "\n",
    "plotter.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6379ea91",
   "metadata": {
    "id": "6379ea91"
   },
   "source": [
    "#### 각 뉴런 별 학습 결과 그래프로 나타내기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87512f1e",
   "metadata": {
    "id": "87512f1e"
   },
   "source": [
    "이번에는 학습을 진행한 $N=10$개의 뉴런으로부터 얻는 각각의 결과값을 그래프로 나타내어 보자. 즉, $i=1, ..., N$의 N개의 뉴런에 대해 구간 $x \\in [-2\\pi, 2\\pi]$에서 $\\varphi(x \\cdot w_{i} + b_{i})$의 그래프가 어떻게 나타나는지 그래프로 그려보면 된다. 각 뉴런의 결과까지만 확인하는 것이므로 이 식에서 $v_i$는 미포함한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb1cf4c",
   "metadata": {
    "id": "deb1cf4c"
   },
   "source": [
    "$N=10$개의 뉴런으로부터 얻는 결과값을 하나의 그래프로 나타내기 위해, subplot이 열 ncols=2, 행 nrows=model.N//2 개인 그래프를 생성할 것이다. 따라서, axes는 원소가 2*(N//2)개인 배열이 된다. 또한 model.parameters[0] (배열 $\\{w_i\\}$)와 model.parameters[1] (배열 $\\{b_i\\}$)은 원소가 $N$개인 배열이다. $N$개의 뉴런에 대응되는 각각의 축에 대해, 정의역 x와 각각의 모델 파라미터들로부터 얻어지는 (x, sigmoid(x * w_i + b_i))를 plot하면 우리가 원하는 결과를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2ca4f5",
   "metadata": {
    "id": "1f2ca4f5"
   },
   "source": [
    "다음의 코드를 완성하여 원하는 $N=10$개의 그래프를 그려보자.\n",
    "\n",
    "참고로, 코드에 나온 flatten() 함수는 다차원 배열을 1차원으로 풀어쓰는 역할을 해주는 함수로, 이론에서 잠깐 나왔던 벡터화(Vectorization)를 수행하는 함수이다. 또한 zip() 함수는 Python의 내장함수로, 여러 개의 iterable한 객체들이 전달되었을 때 각 객체의 요소들을 순서대로 하나씩 뽑아 튜플의 형태로 묶어주는 역할을 한다. 특히 for문에서 활용하기 유용한 함수이다. flatten()과 zip() 함수를 꼭 이용해야만 구현할 수 있는 것은 아니지만, 알고 있으면 더 쉽게 프로그래밍하는 데 도움이 될 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b0601b",
   "metadata": {
    "id": "45b0601b",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, nrows=model.N // 2)\n",
    "x = np.linspace(-2 * np.pi, 2 * np.pi) # 디폴트 : 50개 구간\n",
    "\n",
    "for ax, w, b in zip(axes.flatten(), model.parameters[0].flatten(), model.parameters[1].flatten()):\n",
    "    ax.plot(x, sigmoid(x * w + b)) \n",
    "    ax.grid(\"True\")\n",
    "    ax.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c83eb14",
   "metadata": {
    "id": "1c83eb14"
   },
   "source": [
    "마지막으로 각 뉴런에서 얻어진 결과값에 $v_i$를 곱해준 값들을 하나의 그래프에 plot 하고, 이들을 모두 합한 것을 굵은 검은 점선으로 plot 해보자.  즉, $i=1, ..., N$의 N개의 뉴런에 대해 구간 $x \\in [-2\\pi, 2\\pi]$에서 $v_i\\varphi(x \\cdot w_{i} + b_{i})$의 그래프가 어떻게 나타나는지 그래프로 그려보면 된다. 그리고 이들을 모두 합한 그래프를 함께 그려서, 이 그래프의 모양이 함수 $f(x)=cos(x)$와 얼마나 가까운지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9f9ce0",
   "metadata": {
    "id": "2c9f9ce0",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(-2 * np.pi, 2 * np.pi)\n",
    "F = mg.linspace(0,0)\n",
    "\n",
    "# 하나의 축 위에 모든 그래프를 plot\n",
    "W = model.parameters[0]\n",
    "B = model.parameters[1]\n",
    "V = model.parameters[2]\n",
    "for w, b, v in zip(W.flatten(), B.flatten(), V.flatten()):\n",
    "    ax.plot(x, v * sigmoid(x * w + b))\n",
    "    F += v * sigmoid(x * w + b)\n",
    "    \n",
    "# v * sigmoid(x * w + b)를 모두 합한 것을 함께 plot\n",
    "ax.plot(\n",
    "    x,\n",
    "    F,\n",
    "    color=\"black\",\n",
    "    ls=\"--\",\n",
    "    lw=4,\n",
    "    label=\"full model output\",\n",
    ")\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel(r\"$x$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ceeeb9",
   "metadata": {
    "id": "99ceeeb9"
   },
   "source": [
    "### 배운 내용 되돌아보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45403b77",
   "metadata": {
    "id": "45403b77"
   },
   "source": [
    "이번 실습에서는 Universal Approximation Theorem에 의거하여 함수 의 근사함수 의 모델을 로 세우고 학습을 진행하였다. 우리가 학습시킨 모델은 입력층과 출력층의 뉴런 수가 모두 1인 매우 간단한 형태의 신경망 모델이었지만, 앞으로 작성할 다양한 형태의 신경망의 기본이 되는 중요한 개념들이 많이 나왔다.\n",
    "\n",
    "- batch의 개념을 적용하여 밀집층의 순전파를 직접 프로그래밍하였다. 근사함수 수식을 코딩으로 구현한 결과는 딱 한 줄이었지만, 그 한 줄이 잘 성립하는 코드인지를 확인하기 위해 배열의 연산을 하나씩 따라가며 자세히 살펴보았다.\n",
    "\n",
    "- 이렇게 작성한 순전파 코드를 바탕으로 모델(클래스)를 정의하였다. 하이퍼파라미터인 은닉층의 뉴런 개수는 사용자가 입력하도록 하였고, 모델 파라미터는 표준정규분포를 따르도록 초기화하였다.\n",
    "\n",
    "- 손실함수를 잔차 절댓값의 평균으로 정의하였다. batch에 대해 평균적인 손실을 구하여 학습에 사용함으로써 batch의 이점을 한 번 더 확인하였다.\n",
    "\n",
    "- 경사하강법 함수는 실습 3과 실습 4에서 작성했던 함수와 동일하게 다시 작성하며 복습하였다.\n",
    "\n",
    "- 근사하고자 하는 목표 함수가 로 명확히 주어졌기 때문에,  데이터 (train_data) 만 정해주면 함수를 이용하여 데이터셋의  데이터 (truth)를 쉽게 얻을 수 있었다. 이때,  데이터들에 대한 편미분 계수가 계산되지 않도록 를 상수 취급해야 했다.\n",
    "\n",
    "- 학습을 진행할 때는 랜덤 batch를 구성하여 각 batch에 대해 iteration을 진행해야 했다. 이때, 랜덤 batch는 랜덤하게 섞인 인덱스 배열을 이용하여 train_data 배열에 랜덤하게 접근하는 방식으로 사용하였다.\n",
    "\n",
    "- 1 iteration 동안 실제(truth)값과 모델의 순전파로 얻은 예측(prediction)값을 손실함수에 대입한 후, 손실함수에 대해 자동미분을 이용한 경사하강법을 진행하여 모델 파라미터를 1회 업데이트하였다. 모든 batch에 대해 iteration을 진행하는 epoch를 총 1000회 진행함으로써 모델을 효과적으로 학습시켰다.\n",
    "\n",
    "- 학습 전과 후의 목표 함수 와 근사함수 를 비교해보았고, 학습이 진행됨에 따라 가 에 가까워지는 과정을 관찰해보기도 했다.\n",
    "\n",
    "- 학습이 진행됨에 따라 손실 loss가 실시간으로 감소하는 것을 noggin 라이브러리를 이용하여 관찰하였다.\n",
    "\n",
    "- 학습이 완료된 후, 은닉층의 각 뉴런들이 어떤 결과를 도출하는지 확인해보았다. 또한, 각 뉴런에서 도출한 결과에 가중치 연산을 한 결과와 그렇게 구한 모든 함수를 합한 결과를 함께 시각화함으로써 뉴런 단위로 학습 결과가 어떻게 나타났는지 살펴보았다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML_실습5.ipynb",
   "provenance": []
  },
  "jupytext": {
   "notebook_metadata_filter": "nbsphinx"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
